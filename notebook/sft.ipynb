{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f526208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, re, random, glob, json, jieba, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    TextGenerationPipeline\n",
    ")\n",
    "\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "from sys import platform\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    # linux\n",
    "    root = \"/mnt/sfevol775196/sunzeye273/Data\"\n",
    "#     root = \"/mnt/pa002-28359-vol543625-private/Data\"\n",
    "#     root = \"/root/autodl-tmp/Data\"\n",
    "elif platform == \"darwin\":\n",
    "    # OS X\n",
    "    root = \"/Users/zeyesun/Documents/Data\"\n",
    "elif platform == \"win32\":\n",
    "    # Windows...\n",
    "    root = \"D:\\\\Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_TEXT_PATTERN = re.compile(r\"[\\r\\n]\")\n",
    "\n",
    "def clean_text(text):\n",
    "    return CLEAN_TEXT_PATTERN.sub(\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e69a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"pangu-350M\"\n",
    "# model_name = \"glm-350M-chinese\"\n",
    "model_name = \"chatglm-6B\"\n",
    "model_name_or_path = os.path.join(root, \"models\", model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_cache=False, trust_remote_code=True)\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.all_special_ids)\n",
    "print(\n",
    "    f\"unk: {tokenizer.unk_token_id}\\n\",\n",
    "    f\"pad: {tokenizer.pad_token_id}\\n\",\n",
    "    f\"bos: {tokenizer.bos_token_id}\\n\",\n",
    "    f\"eos: {tokenizer.eos_token_id}\\n\",\n",
    "    f\"sep: {tokenizer.sep_token_id}\\n\",\n",
    "    f\"mask: {tokenizer.mask_token_id}\\n\",\n",
    "#     f\"eop: {tokenizer.eop_token_id}\\n\"\n",
    "#     f\"sop: {tokenizer.sop_token_id}\\n\"\n",
    "#     f\"cls: {tokenizer.cls_token_id}\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cae21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"glm\" in model_name_or_path:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "    if \"chatglm\" in model_name_or_path:\n",
    "        model = model.half()\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, use_cache=False)\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "model.to(device)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28f07a",
   "metadata": {},
   "source": [
    "# SFT Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_files = os.path.join(root, \"chatgpt\", \"output\", \"sft\", \"pangu-350M\", \"checkpoint-57043\", \"pytorch_model*.bin\")\n",
    "# checkpoint_files = os.path.join(root, \"chatgpt\", \"output\", \"sft\", \"pangu-2.6B\", \"external_checkpoint-9000\", \"pytorch_model*.bin\")\n",
    "checkpoints = glob.glob(checkpoint_files)\n",
    "st = dict()\n",
    "for checkpoint in checkpoints:\n",
    "    st.update(torch.load(checkpoint, map_location=\"cpu\"))\n",
    "model.load_state_dict(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "max_length_generation = 50\n",
    "num_return_sequences = 1\n",
    "top_p = 0.8\n",
    "temperature = 0.8\n",
    "# prompt = \"写一篇歌颂祖国的文章\"\n",
    "prompt = '今天晚上我在睡觉.........他想要做那些事..我就大大声骂他\"不要吵我睡觉\"!!!!!...他就跑出去了...还不接我电话'\n",
    "# prompt = \"上联：东风执笔点龙睛，看幸福指数，天天向上\"\n",
    "# prompt = \"中美关系进一步恶化\"\n",
    "# prompt = \"\"\"阅读文章：《战国无双3》（）是由光荣和ω-force开发的战国无双系列的正统第三续作。本作以三大故事为主轴，分别是以武田信玄等人为主的《关东三国志》，织田信长等人为主的《战国三杰》，石田三成等人为主的《关原的年轻武者》，丰富游戏内的剧情。此部份专门介绍角色，欲知武器情报、奥义字或擅长攻击类型等，请至战国无双系列1.由于乡里大辅先生因故去世，不得不寻找其他声优接手。从猛将传 and Z开始。2.战国无双 编年史的原创男女主角亦有专属声优。此模式是任天堂游戏谜之村雨城改编的新增模式。本作中共有20张战场地图（不含村雨城），后来发行的猛将传再新增3张战场地图。但游戏内战役数量繁多，部分地图会有兼用的状况，战役虚实则是以光荣发行的2本「战国无双3 人物真书」内容为主，以下是相关介绍。（注：前方加☆者为猛将传新增关卡及地图。）合并本篇和猛将传的内容，村雨城模式剔除，战国史模式可直接游玩。主打两大模式「战史演武」&「争霸演武」。系列作品外传作品\\n问：《战国无双3》是由哪两个公司合作开发的？\"\"\"\n",
    "# prefix = \"下联：\"\n",
    "prefix = \"答：\"\n",
    "while True:\n",
    "    if \"chatglm\" in model_name_or_path:\n",
    "        encoded_prompt = tokenizer(prompt, prefix + tokenizer.mask_token)\n",
    "        prompt_length = len(encoded_prompt['input_ids'])\n",
    "        inputs = tokenizer(prompt, prefix + tokenizer.mask_token,\n",
    "#                            max_length=max_length - max_length_generation,\n",
    "#                            padding=\"max_length\",\n",
    "                           max_length=min(prompt_length, max_length),\n",
    "                           truncation=\"only_first\",\n",
    "                           return_tensors=\"pt\",\n",
    "                           return_token_type_ids=False)\n",
    "        max_length_generation = max_length - inputs['input_ids'].shape[1]\n",
    "#         inputs_glm = tokenizer.build_inputs_for_generation(inputs, \n",
    "#                                                            max_gen_length=max_length_generation, padding=True)\n",
    "        inputs_glm = inputs_glm.to(device)\n",
    "        outputs = model.generate(**inputs_glm,\n",
    "                                 max_new_tokens=max_length_generation,\n",
    "                                 eos_token_id=tokenizer.eop_token_id,\n",
    "                                 pad_token_id=tokenizer.pad_token_id,\n",
    "                                 do_sample=False,\n",
    "                                 num_return_sequences=num_return_sequences,\n",
    "                                 top_p=top_p,\n",
    "                                 temperature=temperature)\n",
    "    elif \"glm\" in model_name_or_path:\n",
    "        encoded_prompt = tokenizer(prompt, prefix + tokenizer.mask_token)\n",
    "        prompt_length = len(encoded_prompt['input_ids'])\n",
    "        inputs = tokenizer(prompt, prefix + tokenizer.mask_token,\n",
    "#                            max_length=max_length - max_length_generation,\n",
    "#                            padding=\"max_length\",\n",
    "                           max_length=min(prompt_length, max_length),\n",
    "                           truncation=\"only_first\",\n",
    "                           return_tensors=\"pt\",\n",
    "                           return_token_type_ids=False)\n",
    "        max_length_generation = max_length - inputs['input_ids'].shape[1]\n",
    "        inputs_glm = tokenizer.build_inputs_for_generation(inputs, \n",
    "                                                           max_gen_length=max_length_generation, padding=True)\n",
    "        inputs_glm = inputs_glm.to(device)\n",
    "        outputs = model.generate(**inputs_glm,\n",
    "                                 max_new_tokens=max_length_generation,\n",
    "                                 eos_token_id=tokenizer.eop_token_id,\n",
    "                                 pad_token_id=tokenizer.pad_token_id,\n",
    "                                 do_sample=False,\n",
    "                                 num_return_sequences=num_return_sequences,\n",
    "                                 top_p=top_p,\n",
    "                                 temperature=temperature)\n",
    "    else:\n",
    "        inputs = tokenizer(prompt, tokenizer.sep_token + prefix, \n",
    "                           max_length=max_length, \n",
    "                           return_tensors=\"pt\",\n",
    "                           truncation=\"only_first\", \n",
    "#                            padding=\"max_length\",\n",
    "                           add_special_tokens=False,\n",
    "                           return_token_type_ids=False)\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=max_length_generation,\n",
    "                                 pad_token_id=tokenizer.pad_token_id,\n",
    "                                 do_sample=False,\n",
    "                                 num_return_sequences=num_return_sequences,\n",
    "                                 top_p=top_p,\n",
    "                                 temperature=temperature)\n",
    "    results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "#     results = [result.split(prefix, maxsplit=1)[1] for result in results]\n",
    "    print(results)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = os.path.join(root, \"raw\", \"baike_qa_train.json\")\n",
    "# [baike_qa.jsonl, chinese_classical.jsonl, chinese_poetry.jsonl, couplets.jsonl, weibo_summary_comments.jsonl, zhidao.jsonl]\n",
    "f = os.path.join(root, \"chatgpt\", \"processed\", \"baike_qa.jsonl\")\n",
    "i = 0\n",
    "prompts = []\n",
    "prompts_processed = []\n",
    "labels = []\n",
    "with open(f, \"r\", encoding=\"utf-8\") as r:\n",
    "    while True:\n",
    "        line = r.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        item = json.loads(line.strip(\"\\n\"))\n",
    "        # prompt = clean_text(item['title'] if len(item['title']) > len(item['desc']) else item['desc'])\n",
    "        # prompt_processed = prompt + tokenizer.sep_token + prefix\n",
    "        # label = clean_text(item['answer'])\n",
    "        prompt = item['prompt']\n",
    "        prompt_processed = prompt\n",
    "        label = item['answers'][0]['answer']\n",
    "        prompts.append(prompt)\n",
    "        prompts_processed.append(prompt_processed)\n",
    "        labels.append(label)\n",
    "        i += 1\n",
    "        # if i > 1000:\n",
    "        #     break\n",
    "random.shuffle(prompts_processed)\n",
    "print(len(prompts_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad64148",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 79\n",
    "num_return_sequences = 2\n",
    "max_length = 512\n",
    "max_length_generation = 100\n",
    "top_k = 50\n",
    "top_p = 0.8\n",
    "temperature = 1.0\n",
    "t1 = time.time()\n",
    "prompt = prompts_processed[i]\n",
    "inputs = tokenizer(prompt, add_special_tokens=False, return_token_type_ids=False, return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)\n",
    "outputs = model.generate(**inputs,\n",
    "                         max_new_tokens=max_length_generation,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         do_sample=True,\n",
    "                         num_return_sequences=num_return_sequences,\n",
    "                         # top_p=top_p,\n",
    "                         top_k=top_k,\n",
    "                         temperature=temperature)\n",
    "results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for result in results:\n",
    "    # result.split(prefix, maxsplit=1)[1]\n",
    "    print(result)\n",
    "\n",
    "\n",
    "# results = text_generator(prompts_processed[i:j], max_length=200, num_return_sequences=num_return_sequences,\n",
    "#                          do_sample=True, top_k=50, temperature=10.0)\n",
    "# print(f\"Finished prediction, time taken: {time.time()-t1}\")\n",
    "\n",
    "# for prompt, res, label in zip(prompts[i:j], results[:(j-i)], labels[i:j]):\n",
    "#     print(f\"prompt: {prompt}\\nlabel: {label}\")\n",
    "#     for k in range(num_return_sequences):\n",
    "#         model_answer = res[k]['generated_text'].split(prefix)[1].replace(\"<eot>\", \"\").replace(\"<pad>\", \"\")\n",
    "#         print(f\"model answer-{k}: {model_answer}\")\n",
    "#     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e08ce",
   "metadata": {},
   "source": [
    "# SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef5821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"你是谁\"\n",
    "# prefix = \"答:\"\n",
    "# label = \"我是***，很高兴为你服务\"\n",
    "prompt = \"\"\"倍数金额： 1倍，￥1024 元 场次 主---------客队 投注选项- 参考赔率---投注选项胜 平 负---\n",
    "第一次为基本面投注---第二次为通过处理后投注  1 伯明翰 VS -----维冈 31-----1.93 3.27 3.87 ---  2伯恩利VS---朴茨茅30----- ---3 博尔顿 VS -----狼队\n",
    "3------1.94 3.25 3.88 ---  4 斯托克 VS ---阿森纳 0------5.03 3.47 1.68 ---  5 门兴 VS -----弗赖堡 31-----1.77 3.倍数金额： 1倍，￥1024 元 场次\n",
    "主---------客队 投注选项- 参考赔率---投注选项胜 平 负--- 第一次为基本面投注---第二次为通过处理后投注  1 伯明翰 VS -----维冈 31-----1.93 3.27 3.87 ---\n",
    "2伯恩利VS---朴茨茅30----- ---3 博尔顿 VS -----狼队 3------1.94 3.25 3.88 ---  4 斯托克 VS ---阿森纳 0------5.03 3.47 1.68 ---  5 门兴 VS -----弗赖堡\n",
    "31-----1.77 3.39 4.43 ---  6 美因兹 VS ---不来梅 10-----3.76 3.34 1.92 ---  7波鸿VS-----纽伦堡30----- ---8 斯图加 VS ---法兰克 31-----1.59 3.62 5.47\n",
    "---  9 赫塔 VS -----霍芬海 30-----2.49 3.19 2.69 ---  10 勒沃 VS ------科隆 3------1.35 4.44 8.31 ---  11卡塔尼VS----巴里31----- ---12 拉齐奥 VS\n",
    "--佛罗伦 31-----2.35 3.05 3.01 ---  13 特内里 VS ----皇马 0------9.43 4.95 1.29 ---  14 巴萨 VS ----马拉加 3------1.15 6.78 15.49 --\"\"\"\n",
    "prefix = \"回答：\"\n",
    "label = \"你出的赔率数据太早了，数据随时都会变化，这就是所谓要看临盘的道理，目前的数据没什么参考价值。\"\n",
    "max_length = 512\n",
    "encoded_prompt = tokenizer(prompt, prefix + tokenizer.mask_token)\n",
    "prompt_length = len(encoded_prompt['input_ids'])\n",
    "label_length = len(tokenizer.tokenize(label)) + (1 if \"chatglm\" not in model_name_or_path else 0)\n",
    "# print(f\"prompt length: {prompt_length}, label length: {label_length}\")\n",
    "if prompt_length + label_length > max_length:\n",
    "    num_tokens_to_remove = prompt_length + label_length - max_length\n",
    "    for _ in range(num_tokens_to_remove):\n",
    "        if prompt_length > label_length:\n",
    "            prompt_length -= 1\n",
    "        else:\n",
    "            label_length -= 1\n",
    "else:\n",
    "    label_length = max_length - prompt_length\n",
    "assert prompt_length > 0\n",
    "assert label_length > 0\n",
    "assert prompt_length + label_length <= max_length\n",
    "encoded_dict = tokenizer(prompt, prefix + tokenizer.mask_token,\n",
    "                         max_length=prompt_length, truncation=\"only_first\",\n",
    "                         return_tensors=\"pt\", return_attention_mask=True)\n",
    "inputs = tokenizer.build_inputs_for_generation(encoded_dict, targets=label,\n",
    "                                               max_gen_length=label_length, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b9e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.all_special_ids)\n",
    "print(\n",
    "    f\"unk: {tokenizer.unk_token_id}\\n\",\n",
    "    f\"pad: {tokenizer.pad_token_id}\\n\",\n",
    "    f\"bos: {tokenizer.bos_token_id}\\n\",\n",
    "    f\"eos: {tokenizer.eos_token_id}\\n\",\n",
    "    f\"sep: {tokenizer.sep_token_id}\\n\",\n",
    "    f\"mask: {tokenizer.mask_token_id}\\n\",\n",
    "#     f\"eop: {tokenizer.eop_token_id}\\n\"\n",
    "#     f\"sop: {tokenizer.sop_token_id}\\n\"\n",
    "#     f\"cls: {tokenizer.cls_token_id}\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c82e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens([20006]))\n",
    "print(tokenizer.convert_ids_to_tokens([20012]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d349a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_prompt)\n",
    "print(tokenizer.decode(encoded_prompt['input_ids']))\n",
    "print(encoded_dict)\n",
    "print(tokenizer.batch_decode(encoded_dict['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in inputs.items():\n",
    "    print(f\"{key} shape: {val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in inputs_glm.items():\n",
    "    print(f\"{key} shape: {val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs_glm['input_ids'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs_glm['labels'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac236f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs_glm['attention_mask'][0][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs_glm['position_ids'][0][:20])\n",
    "print(inputs_glm['position_ids'][1][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83126e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# st = model.state_dict()\n",
    "st.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(st['transformer.word_embeddings.weight'].dtype)\n",
    "print(st['transformer.layers.0.input_layernorm.weight'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81289f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = dict()\n",
    "for key, val in st.items():\n",
    "    if val.dtype not in dtypes:\n",
    "        dtypes[val.dtype] = list()\n",
    "    dtypes[val.dtype].append(key)\n",
    "print(dtypes.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e3327",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc223bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
