{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/root/autodl-tmp/Code/RLHF\")\n",
    "sys.path.insert(0, \"/Users/zeyesun/Documents/Code/RLHF\")\n",
    "sys.path.insert(0, \"D:\\\\Code\\\\RLHF\")\n",
    "sys.path.insert(0, \"/mnt/sfevol775196/sunzeye273/Code/chatgpt\")\n",
    "sys.path.insert(0, \"/mnt/share-pa002-vol682688-prd/sunzeye273/Code/chatgpt\")\n",
    "sys.path.insert(0, \"/mnt/pa002-28359-vol543625-private/Code/chatgpt\")\n",
    "\n",
    "import os, time, re, random, glob, json, jieba, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    TextGenerationPipeline\n",
    ")\n",
    "\n",
    "from src.models.reward import RewardModel\n",
    "\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "from sys import platform\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    # linux\n",
    "    root = \"/mnt/sfevol775196/sunzeye273/Data\"\n",
    "#     root = \"/mnt/share-pa002-vol682688-prd/sunzeye273/Data\"\n",
    "#     root = \"/mnt/pa002-28359-vol543625-private/Data\"\n",
    "#     root = \"/root/autodl-tmp/Data\"\n",
    "elif platform == \"darwin\":\n",
    "    # OS X\n",
    "    root = \"/Users/zeyesun/Documents/Data\"\n",
    "elif platform == \"win32\":\n",
    "    # Windows...\n",
    "    root = \"D:\\\\Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354bbf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"pangu-small\"\n",
    "# model_name = \"pangu-350M\"\n",
    "# model_name = \"glm-small\"\n",
    "model_name = \"chatglm-6B\"\n",
    "model_name_or_path = os.path.join(root, \"models\", model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_cache=False, trust_remote_code=True)\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.all_special_ids)\n",
    "print(\n",
    "    f\"unk: {tokenizer.unk_token_id}\\n\",\n",
    "    f\"pad: {tokenizer.pad_token_id}\\n\",\n",
    "    f\"bos: {tokenizer.bos_token_id}\\n\",\n",
    "    f\"eos: {tokenizer.eos_token_id}\\n\",\n",
    "    f\"sep: {tokenizer.sep_token_id}\\n\",\n",
    "    f\"mask: {tokenizer.mask_token_id}\\n\",\n",
    "    f\"eop: {tokenizer.eop_token_id}\\n\"\n",
    "#     f\"sop: {tokenizer.sop_token_id}\\n\"\n",
    "    f\"cls: {tokenizer.cls_token_id}\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"glm\" in model_name_or_path:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "    if \"chatglm\" in model_name_or_path:\n",
    "        model = model.half()\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, use_cache=False)\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "if lora_rank > 0:\n",
    "    convert_to_lora_recursively(model, lora_rank, lora_alpha)\n",
    "    lora.mark_only_lora_as_trainable(model, lora_train_bias)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1484a82",
   "metadata": {},
   "source": [
    "## Dataset Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469acb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data import SFTDataset\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "args = {'model_name_or_path': model_name_or_path,\n",
    "        \"max_length\": 128}\n",
    "args = dotdict(args)\n",
    "\n",
    "train_dataset = SFTDataset(args, \"/Users/zeyesun/Documents/Data/chatgpt/processed/test_data_external_v1.jsonl\", \n",
    "                              tokenizer)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b621de",
   "metadata": {},
   "source": [
    "## Generation Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc48eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"你好，你是谁？\"\n",
    "prefix = \"答:\"\n",
    "label = \"我是ChatGPT\"\n",
    "max_length = 32\n",
    "max_gen_length = 16\n",
    "max_prompt_length = max_length - max_gen_length\n",
    "lora_rank = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "inputs = tokenizer(prompt, \n",
    "#                    label, \n",
    "#                    tokenizer.sep_token + prefix,\n",
    "                   max_length=max_prompt_length,\n",
    "                   padding=\"max_length\",\n",
    "                   truncation=\"longest_first\", \n",
    "#                    add_special_tokens=False,\n",
    "                   return_tensors=\"pt\", \n",
    "                   return_token_type_ids=False)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce652f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, prompt_length = inputs['input_ids'].shape\n",
    "\n",
    "with torch.no_grad():\n",
    "#     logger.debug(f\"[_generate_sequence] inputs: {inputs}\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    seq = model.generate(**inputs, \n",
    "                         max_new_tokens=max_gen_length,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         do_sample=False,\n",
    "                         num_return_sequences=1,\n",
    "                         top_p=0.9,\n",
    "                         temperature=1.0\n",
    "                        )\n",
    "print(f\"seq: {seq}\")\n",
    "print(tokenizer.batch_decode(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for i in range(batch_size):\n",
    "    prompt_ids = seq[i, :prompt_length]\n",
    "    prompt_start_index = (prompt_ids != tokenizer.pad_token_id).nonzero()[0].item()\n",
    "    prompt_ids = seq[i, prompt_start_index:prompt_length]\n",
    "    answer_ids = seq[i, prompt_length:]\n",
    "    prompt = tokenizer.decode(prompt_ids, skip_special_tokens=False)\n",
    "    answer = tokenizer.decode(answer_ids, skip_special_tokens=False)\n",
    "    prompts.append(prompt + answer)\n",
    "print(prompts)\n",
    "outputs = tokenizer(prompts, max_length=max_length,\n",
    "                              truncation=\"longest_first\", padding=\"max_length\",\n",
    "                              return_tensors=\"pt\", return_token_type_ids=False)\n",
    "print(outputs)\n",
    "print(tokenizer.batch_decode(outputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429917ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['input_ids'].device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca3132",
   "metadata": {},
   "source": [
    "## GLM attention mask and position ids Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3308b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatglm build attention mask\n",
    "input_ids = inputs['input_ids']\n",
    "batch_size, seq_length = input_ids.shape\n",
    "context_lengths = [seq.tolist().index(tokenizer.bos_token_id) for seq in input_ids]\n",
    "attention_mask = torch.ones((batch_size, seq_length, seq_length), device=device)\n",
    "print(attention_mask.shape)\n",
    "attention_mask.tril_()\n",
    "for i, context_length in enumerate(context_lengths):\n",
    "    attention_mask[i, :, :context_length] = 1\n",
    "print(attention_mask.shape)\n",
    "attention_mask.unsqueeze_(1)\n",
    "print(attention_mask.shape)\n",
    "# attention_mask = (attention_mask < 0.5).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4fd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatglm bulid position ids\n",
    "batch_size, seq_length = input_ids.shape\n",
    "context_lengths = [seq.tolist().index(tokenizer.bos_token_id) for seq in input_ids]\n",
    "# if self.position_encoding_2d:\n",
    "position_ids = torch.arange(seq_length, dtype=torch.long, device=device).expand(batch_size, seq_length)\n",
    "# if not gmask:\n",
    "#     for i, context_length in enumerate(context_lengths):\n",
    "#         position_ids[i, context_length:] = mask_positions[i]\n",
    "block_position_ids = [torch.cat((\n",
    "    torch.zeros(context_length, dtype=torch.long, device=device),\n",
    "    torch.arange(seq_length - context_length, dtype=torch.long, device=device) + 1\n",
    ")) for context_length in context_lengths]\n",
    "block_position_ids = torch.stack(block_position_ids, dim=0)\n",
    "position_ids = torch.stack((position_ids, block_position_ids), dim=1)\n",
    "# else:\n",
    "#     position_ids = torch.arange(seq_length, dtype=torch.long, device=device).expand(batch_size, seq_length)\n",
    "#     if not gmask:\n",
    "#         for i, context_length in enumerate(context_lengths):\n",
    "#             position_ids[context_length:] = mask_positions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2e572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
