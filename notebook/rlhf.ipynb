{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/root/autodl-tmp/Code/RLHF\")\n",
    "sys.path.insert(0, \"/Users/zeyesun/Documents/Code/RLHF\")\n",
    "sys.path.insert(0, \"D:\\\\Code\\\\RLHF\")\n",
    "sys.path.insert(0, \"/mnt/sfevol775196/sunzeye273/Code/chatgpt\")\n",
    "sys.path.insert(0, \"/mnt/share-pa002-vol682688-prd/sunzeye273/Code/chatgpt\")\n",
    "sys.path.insert(0, \"/mnt/pa002-28359-vol543625-private/Code/chatgpt\")\n",
    "\n",
    "import os, time, re, random, glob, json, jieba, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    TextGenerationPipeline\n",
    ")\n",
    "\n",
    "from src.models.reward import RewardModel\n",
    "\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "from sys import platform\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    # linux\n",
    "    root = \"/mnt/sfevol775196/sunzeye273/Data\"\n",
    "#     root = \"/mnt/share-pa002-vol682688-prd/sunzeye273/Data\"\n",
    "#     root = \"/mnt/pa002-28359-vol543625-private/Data\"\n",
    "#     root = \"/root/autodl-tmp/Data\"\n",
    "elif platform == \"darwin\":\n",
    "    # OS X\n",
    "    root = \"/Users/zeyesun/Documents/Data\"\n",
    "elif platform == \"win32\":\n",
    "    # Windows...\n",
    "    root = \"D:\\\\Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354bbf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pangu-small\"\n",
    "# model_name = \"pangu-350M\"\n",
    "# model_name = \"glm-small\"\n",
    "# model_name = \"chatglm-6B\"\n",
    "model_name_or_path = os.path.join(root, \"models\", model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_cache=False, trust_remote_code=True)\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.all_special_ids)\n",
    "print(\n",
    "    f\"unk: {tokenizer.unk_token_id}\\n\",\n",
    "    f\"pad: {tokenizer.pad_token_id}\\n\",\n",
    "    f\"bos: {tokenizer.bos_token_id}\\n\",\n",
    "    f\"eos: {tokenizer.eos_token_id}\\n\",\n",
    "    f\"sep: {tokenizer.sep_token_id}\\n\",\n",
    "    f\"mask: {tokenizer.mask_token_id}\\n\",\n",
    "#     f\"eop: {tokenizer.eop_token_id}\\n\"\n",
    "#     f\"sop: {tokenizer.sop_token_id}\\n\"\n",
    "#     f\"cls: {tokenizer.cls_token_id}\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"你好，你是谁？\"\n",
    "prefix = \"答:\"\n",
    "label = \"我是ChatGPT\"\n",
    "max_length = 128\n",
    "max_gen_length = 16\n",
    "max_prompt_length = max_length - max_gen_length\n",
    "lora_rank = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"glm\" in model_name_or_path:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "    if \"chatglm\" in model_name_or_path:\n",
    "        model = model.half()\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, use_cache=False)\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "if lora_rank > 0:\n",
    "    convert_to_lora_recursively(model, lora_rank, lora_alpha)\n",
    "    lora.mark_only_lora_as_trainable(model, lora_train_bias)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469acb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data import SFTDataset\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "args = {'model_name_or_path': model_name_or_path,\n",
    "              \"max_length\": max_length}\n",
    "args = dotdict(args)\n",
    "\n",
    "train_dataset = SFTDataset(args, \"/Users/zeyesun/Documents/Data/chatgpt/processed/test_data_external_v1.jsonl\", \n",
    "                              tokenizer)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "inputs = tokenizer(prompt, tokenizer.sep_token + prefix,\n",
    "                   max_length=max_prompt_length,\n",
    "                   padding=\"max_length\",\n",
    "                   truncation=\"longest_first\", \n",
    "                   add_special_tokens=False,\n",
    "                   return_tensors=\"pt\", \n",
    "                   return_token_type_ids=False)\n",
    "\n",
    "batch_size, prompt_length = inputs['input_ids'].shape\n",
    "\n",
    "with torch.no_grad():\n",
    "#     logger.debug(f\"[_generate_sequence] inputs: {inputs}\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    seq = model.generate(**inputs, \n",
    "                         max_new_tokens=max_gen_length,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         do_sample=False,\n",
    "                         num_return_sequences=1,\n",
    "                         top_p=0.9,\n",
    "                         temperature=1.0\n",
    "                        )\n",
    "print(f\"seq: {seq}\")\n",
    "print(tokenizer.batch_decode(seq))\n",
    "# Filter out seq with no asnwers (or very short). This happens when users directly use the pre-training ckpt without supervised finetuning\n",
    "# NOTE: this will causes each GPU has different number of examples\n",
    "\n",
    "# print(f\"prompt_length: {prompt_length}\")\n",
    "# ans = seq[:, prompt_length:]\n",
    "# print(f\"ans: {ans}\")\n",
    "# valid_ans_len = (ans != tokenizer.pad_token_id).sum(dim=-1)\n",
    "# print(f\"valid_ans_len: {valid_ans_len}\")\n",
    "# out_seq = []\n",
    "# for i in range(batch_size):\n",
    "#     # if the answer is shorter than 1 token, drop it\n",
    "#     if valid_ans_len[i] <= 1:\n",
    "#         continue\n",
    "#     else:\n",
    "#         out_seq.append(seq[i:i + 1])\n",
    "# out_seq = torch.cat(out_seq, dim=0) \n",
    "# print(f\"out_seq: {out_seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for i in range(batch_size):\n",
    "    prompt_ids = seq[i, :prompt_length]\n",
    "    prompt_start_index = (prompt_ids != tokenizer.pad_token_id).nonzero()[0].item()\n",
    "    prompt_ids = seq[i, prompt_start_index:prompt_length]\n",
    "    answer_ids = seq[i, prompt_length:]\n",
    "    prompt = tokenizer.decode(prompt_ids, skip_special_tokens=False)\n",
    "    answer = tokenizer.decode(answer_ids, skip_special_tokens=False)\n",
    "    prompts.append(prompt + answer)\n",
    "print(prompts)\n",
    "outputs = tokenizer(prompts, max_length=max_length,\n",
    "                              truncation=\"longest_first\", padding=\"max_length\",\n",
    "                              return_tensors=\"pt\", return_token_type_ids=False)\n",
    "print(outputs)\n",
    "print(tokenizer.batch_decode(outputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429917ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['input_ids'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_id = inputs['input_ids'][0]\n",
    "print(chosen_id)\n",
    "seq_len = len(chosen_id)\n",
    "c_inds = (chosen_id == tokenizer.pad_token_id).nonzero()\n",
    "c_ind = c_inds[0].item() if len(c_inds) > 0 else seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_token = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# prompt_token[\"input_ids\"] = prompt_token[\"input_ids\"]\n",
    "# prompt_token[\"attention_mask\"] = prompt_token[\"attention_mask\"]\n",
    "# print(prompt_token)\n",
    "print(length - (max_length - 1))\n",
    "for key_word in [\"input_ids\", \"attention_mask\"]:\n",
    "    length = prompt_token[key_word].size()[-1]\n",
    "    if length > max_seq_len:\n",
    "        y = prompt_token[key_word].squeeze(0)[length - (max_length - 1):].flip(0)\n",
    "    else:\n",
    "        y = prompt_token[key_word].squeeze(0).flip(0)\n",
    "    prompt_token[key_word] = y\n",
    "# prompt_dataset.append(prompt_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f60362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "data.append([prompt_token[\"input_ids\"], prompt_token[\"attention_mask\"], tokenizer.pad_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41010e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch = {}\n",
    "\n",
    "pad_token_id = data[-1][-1]\n",
    "\n",
    "prompt = pad_sequence([f[0] for f in data],\n",
    "                      padding_value=pad_token_id,\n",
    "                      batch_first=True)\n",
    "prompt_mask = pad_sequence([f[1] for f in data],\n",
    "                           padding_value=0,\n",
    "                           batch_first=True)\n",
    "\n",
    "### make sure the final ouput is a seqence of 2**?\n",
    "length = prompt.size()[-1]\n",
    "pad_length = max_seq_len - length\n",
    "if pad_length > 0:\n",
    "    batch[\"prompt\"] = F.pad(prompt,\n",
    "                            pad=(pad_length, 0),\n",
    "                            mode='constant',\n",
    "                            value=pad_token_id)\n",
    "    batch[\"prompt_att_mask\"] = F.pad(prompt_mask,\n",
    "                                     pad=(pad_length, 0),\n",
    "                                     mode='constant',\n",
    "                                     value=0)\n",
    "else:\n",
    "    batch[\"prompt\"] = prompt\n",
    "    batch[\"prompt_att_mask\"] = prompt_mask\n",
    "batch[\"prompt\"] = batch[\"prompt\"].flip(1)\n",
    "batch[\"prompt_att_mask\"] = batch[\"prompt_att_mask\"].flip(1)\n",
    "# return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2e572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
