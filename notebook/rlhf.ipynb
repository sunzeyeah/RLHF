{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, re, random, glob, json, jieba, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    TextGenerationPipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "from sys import platform\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    # linux\n",
    "    root = \"/mnt/private-pa002-vol726121-prd/Data\"\n",
    "#         root = \"/root/autodl-tmp/Data\"\n",
    "elif platform == \"darwin\":\n",
    "    # OS X\n",
    "    root = \"/Users/zeyesun/Documents/Data\"\n",
    "elif platform == \"win32\":\n",
    "    # Windows...\n",
    "    root = \"D:\\\\Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_TEXT_PATTERN = re.compile(r\"[\\r\\n]\")\n",
    "\n",
    "def clean_text(text):\n",
    "    return CLEAN_TEXT_PATTERN.sub(\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = os.path.join(root, \"models\", \"pangu-350M\")\n",
    "# model_name_or_path = os.path.join(root, \"models\", \"pangu-2.6B\")\n",
    "# model_name_or_path = os.path.join(root, \"models\", \"pangu-13B\")\n",
    "model_name_or_path = os.path.join(root, \"models\", \"glm-335M-chinese\")\n",
    "# model_name_or_path = os.path.join(root, \"models\", \"glm-10B-chinese\")\n",
    "# model_name_or_path = os.path.join(root, \"models\", \"chatglm-6B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_cache=False, trust_remote_code=True)\n",
    "\n",
    "# import sentencepiece\n",
    "# model_file = os.path.join(root, \"models\", \"pangu-350M\", \"vocab.model\")\n",
    "# sp = sentencepiece.SentencePieceProcessor()\n",
    "# sp.Load(model_file=model_file)\n",
    "# for i in range(10):\n",
    "#     print(sp.id_to_piece(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = \"模型回答：\"\n",
    "prefix = \"答:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"glm\" in model_name_or_path:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, use_cache=False)\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "model.to(device)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_files = os.path.join(root, \"chatgpt\", \"output\", \"sft\", \"pangu-350M\", \"external_checkpoint-12000\", \"pytorch_model*.bin\")\n",
    "# checkpoint_files = os.path.join(root, \"chatgpt\", \"output\", \"sft\", \"pangu-2.6B\", \"external_checkpoint-9000\", \"pytorch_model*.bin\")\n",
    "checkpoints = glob.glob(checkpoint_files)\n",
    "st = dict()\n",
    "for checkpoint in checkpoints:\n",
    "    st.update(torch.load(checkpoint, map_location=\"cpu\"))\n",
    "model.load_state_dict(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_generator = TextGenerationPipeline(model, tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "max_length_generation = 50\n",
    "num_return_sequences = 1\n",
    "top_p = 0.8\n",
    "temperature = 0.8\n",
    "prompt = \"写一篇歌颂祖国的文章\"\n",
    "# prompt_processed = prompt + tokenizer.sep_token + prefix\n",
    "# prompt = \"\"\"阅读文章：《战国无双3》（）是由光荣和ω-force开发的战国无双系列的正统第三续作。本作以三大故事为主轴，分别是以武田信玄等人为主的《关东三国志》，织田信长等人为主的《战国三杰》，石田三成等人为主的《关原的年轻武者》，丰富游戏内的剧情。此部份专门介绍角色，欲知武器情报、奥义字或擅长攻击类型等，请至战国无双系列1.由于乡里大辅先生因故去世，不得不寻找其他声优接手。从猛将传 and Z开始。2.战国无双 编年史的原创男女主角亦有专属声优。此模式是任天堂游戏谜之村雨城改编的新增模式。本作中共有20张战场地图（不含村雨城），后来发行的猛将传再新增3张战场地图。但游戏内战役数量繁多，部分地图会有兼用的状况，战役虚实则是以光荣发行的2本「战国无双3 人物真书」内容为主，以下是相关介绍。（注：前方加☆者为猛将传新增关卡及地图。）合并本篇和猛将传的内容，村雨城模式剔除，战国史模式可直接游玩。主打两大模式「战史演武」&「争霸演武」。系列作品外传作品\\n问：《战国无双3》是由哪两个公司合作开发的？\\n答：\"\"\"\n",
    "while True:\n",
    "    if \"glm\" in model_name_or_path:\n",
    "#         prompt += tokenizer.mask_token\n",
    "        prompt += \"[gMASK]\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=max_length + max_length_generation)\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=max_length_generation,\n",
    "                                 eos_token_id=tokenizer.eop_token_id,\n",
    "                                 pad_token_id=tokenizer.pad_token_id,\n",
    "                                 do_sample=False,\n",
    "                                 num_return_sequences=num_return_sequences,\n",
    "                                 top_p=top_p,\n",
    "                                 temperature=temperature)\n",
    "        \n",
    "    else:\n",
    "        inputs = tokenizer(prompt, add_special_tokens=False, return_token_type_ids=False, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=max_length_generation,\n",
    "                                 pad_token_id=tokenizer.pad_token_id,\n",
    "                                 do_sample=False,\n",
    "                                 num_return_sequences=num_return_sequences,\n",
    "                                 top_p=top_p,\n",
    "                                 temperature=temperature)\n",
    "        # outputs = text_generator(prompt, max_length=args.max_length_generation,\n",
    "        #                          do_sample=True, num_return_sequences=args.num_return_sequences,\n",
    "        #                          top_p=args.top_p, temperature=args.temperature)\n",
    "#         results = [output['generated_text'].split(prefix, maxsplit=1)[1].replace(tokenizer.eos_token, \"\").replace(tokenizer.pad_token, \"\") for output in outputs]\n",
    "    results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    results = [result.split(prefix, maxsplit=1)[1] for result in results]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = os.path.join(root, \"raw\", \"baike_qa_train.json\")\n",
    "# [baike_qa.jsonl, chinese_classical.jsonl, chinese_poetry.jsonl, couplets.jsonl, weibo_summary_comments.jsonl, zhidao.jsonl]\n",
    "f = os.path.join(root, \"chatgpt\", \"processed\", \"baike_qa.jsonl\")\n",
    "i = 0\n",
    "prompts = []\n",
    "prompts_processed = []\n",
    "labels = []\n",
    "with open(f, \"r\", encoding=\"utf-8\") as r:\n",
    "    while True:\n",
    "        line = r.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        item = json.loads(line.strip(\"\\n\"))\n",
    "        # prompt = clean_text(item['title'] if len(item['title']) > len(item['desc']) else item['desc'])\n",
    "        # prompt_processed = prompt + tokenizer.sep_token + prefix\n",
    "        # label = clean_text(item['answer'])\n",
    "        prompt = item['prompt']\n",
    "        prompt_processed = prompt\n",
    "        label = item['answers'][0]['answer']\n",
    "        prompts.append(prompt)\n",
    "        prompts_processed.append(prompt_processed)\n",
    "        labels.append(label)\n",
    "        i += 1\n",
    "        # if i > 1000:\n",
    "        #     break\n",
    "random.shuffle(prompts_processed)\n",
    "print(len(prompts_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 79\n",
    "num_return_sequences = 2\n",
    "max_length = 512\n",
    "max_length_generation = 100\n",
    "top_k = 50\n",
    "top_p = 0.8\n",
    "temperature = 1.0\n",
    "t1 = time.time()\n",
    "prompt = prompts_processed[i]\n",
    "inputs = tokenizer(prompt, add_special_tokens=False, return_token_type_ids=False, return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)\n",
    "outputs = model.generate(**inputs,\n",
    "                         max_new_tokens=max_length_generation,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         do_sample=True,\n",
    "                         num_return_sequences=num_return_sequences,\n",
    "                         # top_p=top_p,\n",
    "                         top_k=top_k,\n",
    "                         temperature=temperature)\n",
    "results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for result in results:\n",
    "    # result.split(prefix, maxsplit=1)[1]\n",
    "    print(result)\n",
    "\n",
    "\n",
    "# results = text_generator(prompts_processed[i:j], max_length=200, num_return_sequences=num_return_sequences,\n",
    "#                          do_sample=True, top_k=50, temperature=10.0)\n",
    "# print(f\"Finished prediction, time taken: {time.time()-t1}\")\n",
    "\n",
    "# for prompt, res, label in zip(prompts[i:j], results[:(j-i)], labels[i:j]):\n",
    "#     print(f\"prompt: {prompt}\\nlabel: {label}\")\n",
    "#     for k in range(num_return_sequences):\n",
    "#         model_answer = res[k]['generated_text'].split(prefix)[1].replace(\"<eot>\", \"\").replace(\"<pad>\", \"\")\n",
    "#         print(f\"model answer-{k}: {model_answer}\")\n",
    "#     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split torch checkpoint into multiple checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = os.path.join(root, \"chatgpt\", \"output\", \"sft\", \"pangu-2.6B\", \"checkpoint-42782\")\n",
    "if \"glm\" in model_name_or_path:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True, use_cache=False)\n",
    "st = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(st))\n",
    "keys = list(st.keys())\n",
    "n = 10\n",
    "m = {\"metadata\": {\"total_size\":sys.getsizeof(st)}, \"weight_map\":dict()}\n",
    "span = len(keys) // n\n",
    "for i in range(n):\n",
    "    fn = f\"pytorch_model-{i+1}-of-{n}.bin\"\n",
    "    f = os.path.join(checkpoint, fn)\n",
    "    stt = dict()\n",
    "    for key in keys[i*span:(i+1)*span]:\n",
    "        stt[key] = st[key]\n",
    "        m[\"weight_map\"][key] = fn\n",
    "    torch.save(stt, f)\n",
    "f = os.path.join(checkpoint, \"pytorch_model.bin.index.json\")\n",
    "json.dump(m, open(f, \"w\", encoding=\"utf-8\"), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weibo_summary_comments_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "fi = os.path.join(root, \"raw\", \"weibo_summary_comments_json.json\")\n",
    "fo = os.path.join(root, \"chatgpt\", \"processed\", \"weibo_summary_comments.jsonl\")\n",
    "ct = 0\n",
    "with open(fo, \"w\", encoding=\"utf-8\") as w:\n",
    "    with open(fi, \"r\", encoding=\"utf-8\") as r:\n",
    "        while True:\n",
    "            line = r.readline()\n",
    "            if not line\n",
    "                break\n",
    "            \n",
    "            item = json.loads(line.strip(\"\\n\"))\n",
    "            article = item['article'].replace(\" \", \"\")\n",
    "            abstract = item['abstract'].replace(\" \", \"\")\n",
    "            prompt = f\"新闻内容：{article}{tokenizer.sep_token}摘要：{abstract}{tokenizer.sep_token}评论：\"\n",
    "            answers = [\n",
    "                {\n",
    "                    \"answer\": k.replace(\" \", \"\"), \n",
    "                    \"score\": int(v)\n",
    "                } for (k, v) in sorted(item['comments'], key=lambda x: (int(x[1]), len(x[0])), reverse=True)\n",
    "            ]\n",
    "            w.write(json.dumps({\"prompt\": prompt, \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "            ct += 1\n",
    "print(f\"length: {ct}, time taken: {time.time()-t} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### couplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "fi = os.path.join(root, \"raw\", \"couplets.txt\")\n",
    "fo = os.path.join(root, \"chatgpt\", \"processed\", \"couplets.jsonl\")\n",
    "l2 = []\n",
    "nexts = dict()\n",
    "with open(fi, \"r\", encoding=\"utf-8\") as r:\n",
    "    while True:\n",
    "        line = r.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.strip(\"\\n\")\n",
    "        idx = len(line) // 2\n",
    "        prompt = line[:idx]\n",
    "        answer = line[idx+1:]\n",
    "        answers = [{\"answer\": answer, \"score\": 1}]\n",
    "        l2.append({\"prompt\": f\"上联：{prompt}{tokenizer.sep_token}下联：\", \"answers\": answers})\n",
    "        length = len(answer)\n",
    "        if length not in nexts:\n",
    "            nexts[length] = list()\n",
    "        nexts[length].append(answer)\n",
    "t2 = time.time()\n",
    "print(f\"length: {len(l2)}, # different lengths: {len(nexts)}, time taken: {t2-t1} s\")\n",
    "with open(fo, \"w\", encoding=\"utf-8\") as w:\n",
    "    for i, l in tqdm(enumerate(l2), desc=\"Processing Couplets\"):\n",
    "        answer = l['answers'][0]\n",
    "        length = len(answer['answer'])\n",
    "        # 上下联长度一样\n",
    "        nexts_tmp = set(nexts[length])\n",
    "        nexts_tmp.remove(answer['answer'])\n",
    "        nexts_tmp = set(nexts[length]).difference(set([answer['answer']]))\n",
    "#         nexts_tmp.remove(answer['answer'])\n",
    "        answers.extend([{\"answer\": fa, \"score\": 0} for fa in random.sample(nexts_tmp, 2)])\n",
    "        # 上下联长度不一样\n",
    "        keys = set(nexts.keys())\n",
    "        keys.remove(length)\n",
    "        answers.extend([{\"answer\": random.choice(nexts[key]), \"score\": -1} for key in random.sample(keys, 2)])\n",
    "#         answers = sorted(answers, key=lambda x: x['score'], reverse=True)\n",
    "        w.write(json.dumps({\"prompt\": l['prompt'], \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "#         if i % 1000 == 0:\n",
    "#             print(f\"{i} samples processed, time taken: {time.time()-t2} s\")\n",
    "print(f\"length: {len(l2)}, time taken: {time.time()-t2} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zhidao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "fp = os.path.join(root, \"raw\", \"zhidao\", \"*.csv\")\n",
    "fo = os.path.join(root, \"chatgpt\", \"processed\", \"zhidao.jsonl\")\n",
    "ct = 0\n",
    "with open(fo, \"w\", encoding=\"utf-8\") as w:\n",
    "    for fi in glob.glob(fp):\n",
    "        ct = 0\n",
    "        df = pd.read_csv(fi).sort_values(by=[\"title\", \"is_best\"], ascending=False)\n",
    "        prev_title = None\n",
    "        prev_prompt = None\n",
    "        for _, val in df.iterrows():\n",
    "            if isinstance(val['question'], str) and val['question'] != val['title']:\n",
    "                prompt = f\"问题：{val['title']}{tokenizer.sep_token}内容：{val['question']}{tokenizer.sep_token}回答：\"\n",
    "            else:\n",
    "                prompt = f\"问题：{val['title']}{tokenizer.sep_token}回答：\"\n",
    "            if prev_title is not None and prev_title == val['title']:\n",
    "                answers.append({\"answer\": val['reply'], \"score\": val['is_best']})\n",
    "            else:\n",
    "                if prev_title is not None:\n",
    "#                     l3.append({\"prompt\": prev_prompt, \"answers\": copy.deepcopy(answers)})\n",
    "                    w.write(json.dumps({\"prompt\": prev_prompt, \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "                answers = [{\"answer\": val['reply'], \"score\": val['is_best']}]\n",
    "            prev_prompt = prompt\n",
    "            prev_title = val['title']\n",
    "            ct += 1\n",
    "#         l3.append({\"prompt\": prev_prompt, \"answers\": copy.deepcopy(answers)})\n",
    "        w.write(json.dumps({\"prompt\": prev_prompt, \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "        print(f\"finished processing {os.path.basename(fi)}\")\n",
    "print(f\"length: {ct}, time taken: {time.time()-t} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JDData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "       #Initializing lists\n",
    "        self.start_tags = list()\n",
    "        self.end_tags = list()\n",
    "        self.start_end_tags = list()\n",
    "        self.data_list = list()\n",
    "    #HTML Parser Methods\n",
    "    def handle_starttag(self, startTag, attrs):\n",
    "        self.start_tags.append(startTag)\n",
    "    def handle_endtag(self, endTag):\n",
    "        self.end_tags.append(endTag)\n",
    "    def handle_startendtag(self,startendTag, attrs):\n",
    "        self.start_end_tags.append(startendTag)\n",
    "    def handle_data(self, data):\n",
    "        self.data_list.append(data)\n",
    "        \n",
    "t = time.time()\n",
    "fi = os.path.join(root, \"raw\", \"JDData\", \"*.data*\")\n",
    "# fo = os.path.join(root, \"chatgpt\", \"processed\", \"zhidao.jsonl\")\n",
    "ct = 0\n",
    "with open(fo, \"w\", encoding=\"utf-8\") as w:\n",
    "    for fi in glob.glob(fp):\n",
    "        ct = 0\n",
    "        with open(fi, \"r\", encoding=\"gbk\") as r:\n",
    "            line = r.readline()\n",
    "            items = line.strip(\"\\n\").split(\"\\t\")\n",
    "            parser = MyHTMLParser()\n",
    "            parser.feed(items[1])\n",
    "            for t, d in zip(parser.start_tags, parser.data_list):\n",
    "                print(f\"{t}: {d}\")\n",
    "#                 prompt = f\"问题：{val['title']}{tokenizer.sep_token}内容：{val['question']}{tokenizer.sep_token}回答：\"\n",
    "#                 answers.append({\"answer\": val['reply'], \"score\": val['is_best']})\n",
    "            ct += 1\n",
    "#         l3.append({\"prompt\": prev_prompt, \"answers\": copy.deepcopy(answers)})\n",
    "#         w.write(json.dumps({\"prompt\": prev_prompt, \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "        print(f\"finished processing {os.path.basename(fi)}\")\n",
    "print(f\"length: {ct}, time taken: {time.time()-t} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yf_amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "fi = os.path.join(root, \"raw\", \"yf_amazon\", \"products.csv\")\n",
    "dfp = pd.read_csv(fi)\n",
    "fi = os.path.join(root, \"raw\", \"yf_amazon\", \"ratings.csv\")\n",
    "dfr = pd.read_csv(fi)\n",
    "fi = os.path.join(root, \"raw\", \"yf_amazon\", \"categories.csv\")\n",
    "dfc = pd.read_csv(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.columns\n",
    "# dfp['name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp['cate_id_1'] = dfp['catIds'].apply(lambda x: x.split(\",\")[0])\n",
    "for cid1 in dfp['cate_id_1'].unique():\n",
    "    print(dfc[dfc['catId']==int(cid1)]['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dmsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "fi = os.path.join(root, \"raw\", \"dmsc\", \"movies.csv\")\n",
    "dfm = pd.read_csv(fi)\n",
    "print(dfm.shape)\n",
    "fi = os.path.join(root, \"raw\", \"dmsc\", \"ratings.csv\")\n",
    "dfr = pd.read_csv(fi)\n",
    "print(dfr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr.groupby(\"movieId\", 'rating').count()['comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese Classical-Modern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "fp = os.path.join(root, \"raw\", \"Classical-Modern\", \"bitext\", \"*\")\n",
    "fo = os.path.join(root, \"chatgpt\", \"processed\", \"chinese_classical.jsonl\")\n",
    "l3 = []\n",
    "dicts = dict()\n",
    "for fi in glob.glob(fp):\n",
    "    name = os.path.basename(fi)\n",
    "    dicts[name] = {\"古文\": [], \"现代文\": []}\n",
    "    with open(fi, \"r\", encoding=\"utf-8\") as r:\n",
    "        for i, line in enumerate(r):\n",
    "            line = line.strip(\"\\n\")\n",
    "            if line.startswith(\"古文\"):\n",
    "                p1 = line[3:]\n",
    "                dicts[name]['古文'].append(p1)\n",
    "            elif line.startswith(\"现代文\"):\n",
    "                p2 = line[4:]\n",
    "                dicts[name]['现代文'].append(p2)\n",
    "            elif p1 is not None and p2 is not None:\n",
    "                pair = [(\"古文\", p1), (\"现代文\", p2)]\n",
    "                random.shuffle(pair)\n",
    "                prompt = f\"{pair[0][0]}：{pair[0][1]}{tokenizer.sep_token}{pair[1][0]}：\"\n",
    "                answers = [{\"answer\": pair[1][1], \"score\": 1}]\n",
    "                l3.append({\"prompt\": prompt, \"answers\": answers, \"name\": name})\n",
    "                p1 = None\n",
    "                p2 = None\n",
    "t2 = time.time()\n",
    "print(f\"length: {len(l3)}, # different names: {len(dicts)}, time taken: {t2-t1} s\")\n",
    "with open(fo, \"w\", encoding=\"utf-8\") as w:\n",
    "    for i, l in tqdm(enumerate(l3), desc=\"Processing Chinese Classical-Modern\"):\n",
    "        name = l['name']\n",
    "        prompt = l['prompt']\n",
    "        answer = l['answers'][0]['answer']\n",
    "        if prompt.startswith(\"古文\"):\n",
    "            answer_type = '现代文'\n",
    "        else:\n",
    "            answer_type = '古文'\n",
    "        samples_tmp = set(dicts[name][answer_type])\n",
    "        samples_tmp.remove(answer)\n",
    "        answers.extend([{\"answer\": fa, \"score\": 0} for fa in random.sample(samples_tmp, 2)])\n",
    "        keys = set(dicts.keys())\n",
    "        keys.remove(name)\n",
    "        answers.extend([{\"answer\": random.choice(dicts[key][answer_type]), \"score\": -1} for key in random.sample(keys, 2)])\n",
    "        w.write(json.dumps({\"prompt\": prompt, \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "#         if i % 100 == 0:\n",
    "#             print(f\"{i} samples processed, time taken: {time.time()-t2} s\")\n",
    "print(f\"length: {i}, time taken: {time.time()-t} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese Poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opencc\n",
    "converter = opencc.OpenCC('t2s.json')\n",
    "t1 = time.time()\n",
    "fp = [\n",
    "    # 四书五经\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"lunyu\", \"lunyu.json\"),\n",
    "#     os.path.join(root, \"raw\", \"chinese-poetry\", \"mengxue\", \"*.json\"),\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"sishuwujing\", \"*.json\"),\n",
    "    # 古体诗\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"caocaoshiji\", \"caocao.json\"),\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"shijing\", \"shijing.json\"),\n",
    "    # 楚辞\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"chuci\", \"chuci.json\"),\n",
    "    # 诗\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"shi\", \"poet*.json\"),\n",
    "    # 词\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"ci\", \"ci*.json\"),\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"nalanxingde\", \"*.json\"),\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"wudai\", \"huajianji\", \"*juan.json\"),\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"wudai\", \"nantang\", \"poetrys.json\"),\n",
    "    # 曲\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"yuanqu\", \"yuanqu.json\"),\n",
    "]\n",
    "fs = [each for f in fp for each in glob.glob(f)]\n",
    "\n",
    "l5 = []\n",
    "dicts = dict()\n",
    "for fi in fs:\n",
    "    lines = json.load(open(fi, \"r\", encoding=\"utf-8\"))\n",
    "    if isinstance(lines, dict):\n",
    "        lines = [lines]\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"lunyu\" in fi:\n",
    "            author = \"孔子\"\n",
    "            genre = \"经书\"\n",
    "            title = line['chapter']\n",
    "            contents = \"\".join(line['paragraphs'])\n",
    "        elif \"daxue\" in fi:\n",
    "            author = \"曾子\"\n",
    "            genre = \"经书\"\n",
    "            title = \"大学\"\n",
    "            contents = converter.convert(\"\".join(line['paragraphs'])).replace(\"「\", \"“\").replace(\"」\", \"”\")\n",
    "        elif \"mengzi\" in fi:\n",
    "            author = \"孟子\"\n",
    "            genre = \"经书\"\n",
    "            title = converter.convert(line['chapter'])\n",
    "            contents = converter.convert(\"\".join(line['paragraphs'])).replace(\"「\", \"“\").replace(\"」\", \"”\")\n",
    "        elif \"zhongyong\" in fi:\n",
    "            author = \"孔伋\"\n",
    "            genre = \"经书\"\n",
    "            title = \"中庸\"\n",
    "            contents = converter.convert(\"\".join(line['paragraphs'])).replace(\"「\", \"“\").replace(\"」\", \"”\")\n",
    "        elif \"caocao\" in fi:\n",
    "            author = \"曹操\"\n",
    "            genre = \"古体诗\"\n",
    "            title = line['title']\n",
    "            contents = \"\".join(line['paragraphs'])\n",
    "        elif \"shijing\" in fi:\n",
    "            author = \"诗经\"\n",
    "            genre = \"古体诗\"\n",
    "            title = line['chapter'] + \"-\" + line['section'] + \"-\" + line['title']\n",
    "            contents = \"\".join(line['content'])\n",
    "        elif \"chuci\" in fi:\n",
    "            author = line['author']\n",
    "            genre = \"楚辞\"\n",
    "            title = line['section'] + \"-\" + line['title']\n",
    "            contents = \"\".join(line['content'])\n",
    "        elif \"nalanxingde\" in fi:\n",
    "            author = line['author']\n",
    "            genre = \"词\"\n",
    "            title = line['title']\n",
    "            contents = \"\".join(line['para'])\n",
    "        elif \"huajianci\" in fi:\n",
    "            author = line['author']\n",
    "            genre = \"词\"\n",
    "            title = line['title']\n",
    "            contents = \"\".join(line['paragraphs'])\n",
    "        elif \"nantang\" in fi:\n",
    "            author = line['author']\n",
    "            genre = \"词\"\n",
    "            title = line['title']\n",
    "            contents = \"\".join(line['paragraphs'])\n",
    "        elif \"yuanqu\" in fi:\n",
    "            author = line['author']\n",
    "            genre = \"曲\"\n",
    "            title = line['title']\n",
    "            contents = \"\".join(line['paragraphs'])\n",
    "        elif \"shi\" in fi:\n",
    "            if len(line['paragraphs']) <= 0:\n",
    "                continue\n",
    "            author = converter.convert(line['author'])\n",
    "            genre = \"五言诗\" if len(line['paragraphs'][0]) == 12 else \"七言诗\"\n",
    "            title = converter.convert(line['title'])\n",
    "            contents = converter.convert(\"\".join(line['paragraphs']))\n",
    "        elif \"ci\" in fi:\n",
    "            author = line['author']\n",
    "            genre = \"词\"\n",
    "            title = line['rhythmic']\n",
    "            contents = \"\".join(line['paragraphs'])\n",
    "        if genre not in dicts:\n",
    "            dicts[genre] = dict()\n",
    "        if author not in dicts[genre]:\n",
    "            dicts[genre][author] = dict()\n",
    "        quantifier = \"篇\" if genre in [\"经书\", \"楚辞\"] else \"首\"\n",
    "        prompt = f\"以{author}的风格，写一{quantifier}{genre}，题为{title}{tokenizer.sep_token}\"\n",
    "        answers = [{\"answer\": contents, \"score\": 1}]\n",
    "        l5.append({\"prompt\": prompt, \"answers\": answers, \"genre\": genre, \"title\": title, \"author\": author})\n",
    "        dicts[genre][author][title] = contents\n",
    "        \n",
    "t2 = time.time()\n",
    "print(f\"length: {len(l5)}, # different lengths: {len(dicts)}, time taken: {t2-t1} s\")\n",
    "fo = os.path.join(root, \"chatgpt\", \"processed\", \"chinese_poetry.jsonl\")\n",
    "with open(fo, \"w\", encoding=\"utf-8\") as w:\n",
    "    for i, l in tqdm(enumerate(l5), desc=\"Processing Chinese Poetry\"):\n",
    "        genre = l['genre']\n",
    "        author = l['author']\n",
    "        title = l['title']\n",
    "        prompt = l['prompt']\n",
    "        answers = l['answers']\n",
    "        # 同作者其他作品-2\n",
    "        titles_tmp = set(dicts[genre][author].keys())\n",
    "        titles_tmp.remove(title)\n",
    "        if len(titles_tmp) > 0:\n",
    "            t = random.choice(list(titles_tmp))\n",
    "            answers.append({\"answer\": dicts[genre][author][t], \"score\": 0})\n",
    "        # 同体裁其他作者其他作品-1\n",
    "        authors_tmp = set(dicts[genre].keys())\n",
    "        authors_tmp.remove(author)\n",
    "        a = random.choice(list(authors_tmp))\n",
    "        t = random.choice(list(dicts[genre][a].keys()))\n",
    "        answers.append({\"answer\": dicts[genre][a][t], \"score\": -1})\n",
    "        # 不同体裁作品-0\n",
    "        genres_tmp = set(dicts.keys())\n",
    "        genres_tmp.remove(genre)\n",
    "        g = random.choice(list(genres_tmp))\n",
    "        a = random.choice(list(dicts[g].keys()))\n",
    "        t = random.choice(list(dicts[g][a].keys()))\n",
    "        answers.append({\"answer\": dicts[g][a][t], \"score\": -2})\n",
    "        w.write(json.dumps({\"prompt\": prompt, \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "print(f\"length: {i}, time taken: {time.time()-t2} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baike_qa_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = glob.glob(os.path.join(root, \"raw\", \"baike_qa2019\", \"baike_qa_*.json\"))\n",
    "fo = os.path.join(root, \"chatgpt\", \"processed\", \"baike_qa.jsonl\")\n",
    "ct = 0\n",
    "# items = []\n",
    "# lens_prompt = []\n",
    "# lens_label = []\n",
    "with open(fo, \"w\", encoding=\"utf-8\") as w:\n",
    "    for f in fs:\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as r:\n",
    "            while True:\n",
    "                line = r.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                item = json.loads(line.strip(\"\\n\"))\n",
    "                question = clean_text(item['title'] if len(item['title']) > len(item['desc']) else item['desc'])\n",
    "                prompt = f\"{question}{tokenizer.sep_token}回答：\"\n",
    "                answer = clean_text(item['answer'])\n",
    "                answers = [{\"answer\": answer, \"score\": 1}]\n",
    "#                 items.append(item)\n",
    "#                 lens_prompt.append(len(prompt))\n",
    "#                 lens_label.append(len(label))\n",
    "                w.write(json.dumps({\"prompt\": prompt, \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "                ct += 1\n",
    "print(ct)\n",
    "# print(len(items))\n",
    "# print(np.percentile(lens_prompt, np.arange(90, 101)))\n",
    "# print(np.percentile(lens_label, np.arange(90, 101)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rm-static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = os.path.join(root, \"raw\", \"rm-static\", \"data\", \"test-00000-of-00001-bf4c733542e35fcb.parquet\")\n",
    "df = pd.read_parquet(fi)\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"你好\",\n",
    "    \"晚上睡不着应该怎么办\"\n",
    "]\n",
    "history = []\n",
    "for text in texts:\n",
    "    response, history = model.chat(tokenizer, text, history=history)\n",
    "    print(f\"问: {text}\\n答:{response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
