{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0fa8199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 10:36:01.977849: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/root/autodl-tmp/Code/RLHF\")\n",
    "sys.path.insert(0, \"/Users/zeyesun/Documents/Code/RLHF\")\n",
    "sys.path.insert(0, \"D:\\\\Code\\\\RLHF\")\n",
    "sys.path.insert(0, \"/mnt/sfevol775196/sunzeye273/Code/chatgpt\")\n",
    "sys.path.insert(0, \"/mnt/share-pa002-vol682688-prd/sunzeye273/Code/chatgpt\")\n",
    "sys.path.insert(0, \"/mnt/pa002-28359-vol543625-private/Code/chatgpt\")\n",
    "\n",
    "import os, time, re, random, glob, json, jieba, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    TextGenerationPipeline\n",
    ")\n",
    "\n",
    "from src.models.reward import RewardModel\n",
    "\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "from sys import platform\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    # linux\n",
    "    root = \"/mnt/sfevol775196/sunzeye273/Data\"\n",
    "#     root = \"/mnt/share-pa002-vol682688-prd/sunzeye273/Data\"\n",
    "#     root = \"/mnt/pa002-28359-vol543625-private/Data\"\n",
    "#     root = \"/root/autodl-tmp/Data\"\n",
    "elif platform == \"darwin\":\n",
    "    # OS X\n",
    "    root = \"/Users/zeyesun/Documents/Data\"\n",
    "elif platform == \"win32\":\n",
    "    # Windows...\n",
    "    root = \"D:\\\\Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "354bbf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/wf/12stcn_s6zq56j9h3fnkv5lm0000gn/T/jieba.cache\n",
      "Loading model cost 0.481 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '<eot>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>'}\n",
      "[1, 9, 0, 5, 6]\n",
      "unk: 0\n",
      " pad: 6\n",
      " bos: 1\n",
      " eos: 9\n",
      " sep: 5\n",
      " mask: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"pangu-small\"\n",
    "# model_name = \"glm-small\"\n",
    "# model_name = \"chatglm-6B\"\n",
    "model_name_or_path = os.path.join(root, \"models\", model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_cache=False, trust_remote_code=True)\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.all_special_ids)\n",
    "print(\n",
    "    f\"unk: {tokenizer.unk_token_id}\\n\",\n",
    "    f\"pad: {tokenizer.pad_token_id}\\n\",\n",
    "    f\"bos: {tokenizer.bos_token_id}\\n\",\n",
    "    f\"eos: {tokenizer.eos_token_id}\\n\",\n",
    "    f\"sep: {tokenizer.sep_token_id}\\n\",\n",
    "    f\"mask: {tokenizer.mask_token_id}\\n\",\n",
    "#     f\"eop: {tokenizer.eop_token_id}\\n\"\n",
    "#     f\"sop: {tokenizer.sop_token_id}\\n\"\n",
    "#     f\"cls: {tokenizer.cls_token_id}\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd6a3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"你好，你是谁？\"\n",
    "prefix = \"答:\"\n",
    "label = \"我是ChatGPT\"\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a9d87b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5772,   10,   27,   16,  405,   28,    5, 3330,   17]])\n",
      "tensor([[    1,  5772,    10,    27,    16,   405,    28,     5,    21,    16,\n",
      "           542, 24396,  1122, 21723,     9,     6,     6,     6,     6,     6,\n",
      "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "             6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "             6,     6,     6,     6,     6,     6,     6,     6]])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, tokenizer.sep_token + prefix, max_length=max_length,\n",
    "                                       truncation=\"longest_first\", add_special_tokens=False,\n",
    "                                       return_tensors=\"pt\", return_token_type_ids=False)\n",
    "print(inputs['input_ids'])\n",
    "inputs = tokenizer(prompt, label, max_length=max_length,\n",
    "                                       truncation=\"longest_first\", \n",
    "                   padding=\"max_length\",\n",
    "#                    add_special_tokens=False,\n",
    "                                       return_tensors=\"pt\", \n",
    "                   return_token_type_ids=False)\n",
    "print(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78cd035b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,  5772,    10,    27,    16,   405,    28,     5,    21,    16,\n",
      "          542, 24396,  1122, 21723,     9,     6,     6,     6,     6,     6,\n",
      "            6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "            6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "            6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "            6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "            6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "            6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "            6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "            6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "            6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "            6,     6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "            6,     6,     6,     6,     6,     6,     6,     6])\n"
     ]
    }
   ],
   "source": [
    "chosen_id = inputs['input_ids'][0]\n",
    "print(chosen_id)\n",
    "seq_len = len(chosen_id)\n",
    "c_inds = (chosen_id == tokenizer.pad_token_id).nonzero()\n",
    "c_ind = c_inds[0].item() if len(c_inds) > 0 else seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_token = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# prompt_token[\"input_ids\"] = prompt_token[\"input_ids\"]\n",
    "# prompt_token[\"attention_mask\"] = prompt_token[\"attention_mask\"]\n",
    "# print(prompt_token)\n",
    "print(length - (max_length - 1))\n",
    "for key_word in [\"input_ids\", \"attention_mask\"]:\n",
    "    length = prompt_token[key_word].size()[-1]\n",
    "    if length > max_seq_len:\n",
    "        y = prompt_token[key_word].squeeze(0)[length - (max_length - 1):].flip(0)\n",
    "    else:\n",
    "        y = prompt_token[key_word].squeeze(0).flip(0)\n",
    "    prompt_token[key_word] = y\n",
    "# prompt_dataset.append(prompt_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29f60362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "data.append([prompt_token[\"input_ids\"], prompt_token[\"attention_mask\"], tokenizer.pad_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d41010e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch = {}\n",
    "\n",
    "pad_token_id = data[-1][-1]\n",
    "\n",
    "prompt = pad_sequence([f[0] for f in data],\n",
    "                      padding_value=pad_token_id,\n",
    "                      batch_first=True)\n",
    "prompt_mask = pad_sequence([f[1] for f in data],\n",
    "                           padding_value=0,\n",
    "                           batch_first=True)\n",
    "\n",
    "### make sure the final ouput is a seqence of 2**?\n",
    "length = prompt.size()[-1]\n",
    "pad_length = max_seq_len - length\n",
    "if pad_length > 0:\n",
    "    batch[\"prompt\"] = F.pad(prompt,\n",
    "                            pad=(pad_length, 0),\n",
    "                            mode='constant',\n",
    "                            value=pad_token_id)\n",
    "    batch[\"prompt_att_mask\"] = F.pad(prompt_mask,\n",
    "                                     pad=(pad_length, 0),\n",
    "                                     mode='constant',\n",
    "                                     value=0)\n",
    "else:\n",
    "    batch[\"prompt\"] = prompt\n",
    "    batch[\"prompt_att_mask\"] = prompt_mask\n",
    "batch[\"prompt\"] = batch[\"prompt\"].flip(1)\n",
    "batch[\"prompt_att_mask\"] = batch[\"prompt_att_mask\"].flip(1)\n",
    "# return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2e572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
