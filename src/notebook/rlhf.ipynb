{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, re, random, glob, json, jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    TextGenerationPipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\"\n",
    "# device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_TEXT_PATTERN = re.compile(r\"[\\r\\n]\")\n",
    "\n",
    "def clean_text(text):\n",
    "    return CLEAN_TEXT_PATTERN.sub(\"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece\n",
    "model_file = \"/Users/zeyesun/Documents/Data/models/pangu-350M/vocab.model\"\n",
    "sp = sentencepiece.SentencePieceProcessor()\n",
    "sp.Load(model_file=model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<s>\n",
      "</s>\n",
      "▃\n",
      "▂\n",
      "<sep>\n",
      "<pad>\n",
      "<mask>\n",
      "<eod>\n",
      "<eot>\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(sp.id_to_piece(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_name_or_path = \"D:\\\\Data\\\\models\\\\pangu_2_6B\"\n",
    "model_name_or_path = \"D:\\\\Data\\\\models\\\\pangu-350M\"\n",
    "# model_name_or_path = \"/Users/zeyesun/Documents/Data/models/pangu-350M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'eos_token': \"<eot>\", 'pad_token': \"<pad>\", \"sep_token\": \"<sep>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of GPTPanguForCausalLM were not initialized from the model checkpoint at D:\\Data\\models\\pangu-350M and are newly initialized: ['transformer.h.13.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'lm_head.weight', 'transformer.h.8.attn.bias', 'transformer.h.6.attn.bias', 'transformer.h.22.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.14.attn.bias', 'transformer.h.5.attn.bias', 'transformer.h.10.attn.bias', 'transformer.h.9.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.0.attn.bias', 'transformer.h.16.attn.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.18.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.1.attn.bias', 'transformer.h.15.attn.bias', 'transformer.h.4.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.21.attn.bias', 'transformer.h.19.attn.bias', 'transformer.h.23.attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTPanguForCausalLM(\n",
       "  (transformer): GPTPanguModel(\n",
       "    (wte): Embedding(40000, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (wqe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=40000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, use_cache=False)\n",
    "model.resize_token_embeddings(len(tokenizer.sp))\n",
    "model.config.end_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.max_length_prompt = 200\n",
    "model.to(device)\n",
    "# print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1425170\n"
     ]
    }
   ],
   "source": [
    "# f = \"/Users/zeyesun/Documents/Data/raw/baike_qa2019/baike_qa_train.json\"\n",
    "f = \"D:\\\\Data\\\\raw\\\\baike_qa_train.json\"\n",
    "items = []\n",
    "lens_prompt = []\n",
    "lens_label = []\n",
    "with open(f, \"r\", encoding=\"utf-8\") as r:\n",
    "    while True:\n",
    "        line = r.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        item = json.loads(line.strip(\"\\n\"))\n",
    "        prompt = clean_text(item['title'] if len(item['title']) > len(item['desc']) else item['desc'])\n",
    "        label = clean_text(item['answer'])\n",
    "        items.append(item)\n",
    "        lens_prompt.append(len(prompt))\n",
    "        lens_label.append(len(label))\n",
    "print(len(items))\n",
    "print(np.percentile(lens_prompt, np.arange(90, 101)))\n",
    "print(np.percentile(lens_label, np.arange(90, 101)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 'qid_7770627748113178417',\n",
       " 'category': '生活-保健养生',\n",
       " 'title': '减肥健身我要下载一套很好的有氧健身操，请问哪里有下载地址，谢谢！ ',\n",
       " 'desc': '我要下载一套很好的有氧操，请问哪里有下载地址，谢谢！',\n",
       " 'answer': '我看练太极拳就可以，该运动也是有氧健身的运动，长期锻炼还可以治疗很多慢性病。'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 789\n",
    "items[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我看练太极拳就可以，该运动也是有氧健身的运动，长期锻炼还可以治疗很多慢性病。'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[i]['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint = \"/Users/zeyesun/Documents/Data/output/sft/pangu-350M/checkpoint-12000/pytorch_model.bin\"\n",
    "checkpoint = \"D:\\\\Data\\\\output\\\\sft\\\\pangu-350M\\\\checkpoint-12000\\\\pytorch_model.bin\"\n",
    "st = torch.load(checkpoint, map_location=\"cpu\")\n",
    "model.load_state_dict(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'GPTPanguForCausalLM' is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel'].\n"
     ]
    }
   ],
   "source": [
    "text_generator = TextGenerationPipeline(model, tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = \"/Users/zeyesun/Documents/Data/raw/baike_qa2019/baike_qa_train.json\"\n",
    "f = \"D:\\\\Data\\\\raw\\\\baike_qa_train.json\"\n",
    "i = 0\n",
    "prompts = []\n",
    "prompts_processed = []\n",
    "labels = []\n",
    "with open(f, \"r\", encoding=\"utf-8\") as r:\n",
    "    while True:\n",
    "        line = r.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        item = json.loads(line.strip(\"\\n\"))\n",
    "        prompt = clean_text(item['title'] if len(item['title']) > len(item['desc']) else item['desc'])\n",
    "        label = clean_text(item['answer'])\n",
    "        prompt_processed = prompt + tokenizer.sep_token + \"模型回答:\"\n",
    "        prompts.append(prompt)\n",
    "        prompts_processed.append(prompt_processed)\n",
    "        labels.append(label)\n",
    "        i += 1\n",
    "        if i > 1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: 有6区无尽之海众神之子工会的吗?你们会怎么了?看不见人了,解散了 \n",
      "label: 我听原众神成员说 众神的副会长 卷了工会财产跑路了(几千W呢)  所以导致工会解散 哎 真是人心陷恶   \n",
      "model answer-0: 我也是无尽之海的,现在工会还没解散,是不可能解散了,不过我想你应该有很多个工会吧,应该有很多人在等着你,如果你有足够的耐心的话,可以去工会看看有多少人在等着你,我有个朋友就在等着我,我说的是在无尽之海,你知道的,要是没人的话,你就去工会看看,反正现在工会也没解散,你可以去公会看看,我也是在无尽之海的,在无尽之海也有很多人在等着你,要是你能坚持的话,我想你会更好的!结舌\n",
      "model answer-1: 现在工会也解散了,是吧··タタタ瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄\n",
      "model answer-2: 你可以去看看万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛NdF万盛NdF\n",
      "model answer-3: 不解散了,我是6区无尽之海的,我想知道,如果解散的话,工会的人和工会的人一起走,会怎么样?方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒\n",
      "model answer-4: 有,因为工会的人太多了,服务器都快封了。タ榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "\n",
      "\n",
      "\n",
      "prompt: 老师好，青青稞酒明天再跌就可以建仓了吗？谢谢。 \n",
      "label: 青青稞酒：散户在不断地卖出，主力资金却在周三悄悄地吃便宜货。该股处于上市以来的最低迷状态，这一点从成交量就可以看得清清楚楚。但是，现在也许并不是你介入的最佳时机。因为从形态看，散户还会有更便宜的筹码在以后卖出，而主力资金至今收集的筹码也不足以使其达到建完仓的地步。因此建议你再等等。这只股票将来看好是不言而喻的。对于短线操作来说，介入的标志是放量。对于中长线来说，还要看大趋势。\n",
      "model answer-0: 您好! 青青稞酒的主力成本是16.5元,目前股价已经下跌到了成本价,如果明天继续下跌,将有可能跌破成本价,建议您可以考虑在回调到成本价附近介入,有可能获利。榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "model answer-1: 如果你的资金不多,建议观望,再来一次,我给你推荐一只股票,就是中信证券,我买过,现在是10.4元,现在还在下跌,还可以建仓,我昨天就告诉你了,你可以去看看。ativelady>ativelady>ativelady>ativelady>ativelady>ativelady>ativelady>ativelady>ativelady>ative\n",
      "model answer-2: 我建议明天再跌,再建仓,到了明天再买周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清\n",
      "model answer-3: 建仓可以,但要控制仓位.周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清\n",
      "model answer-4: 明天再跌就可以建仓了,这是我的看法,你可以参考。榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "\n",
      "\n",
      "\n",
      "prompt: onthesouthofthecity是在城内还是成外？ \n",
      "label: 在城外.例如: Viet Nam is on the south of China.越南在中国的南面. 摘录自英汉综合大词典,该字典还有这样两个例句: Spain is in the south of Europe.西班牙在欧洲的南部。Mexico is to the south of the USA. 墨西哥在美国的南部。\n",
      "model answer-0: the city                                           益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰<\n",
      "model answer-1: 在城内,在成外.在城内,在成外.NdFequipmentNdFequipmentNdFequipmentNdFequipment NdFequipmentNdFequipment NdFequipmentNdFequipment NdFequipment NdFequipment NdF\n",
      "model answer-2: 在城内吧!官宣咯!官宣咯!官宣咯!官宣咯!官宣咯!官宣咯!官宣咯!戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛\n",
      "model answer-3: It's not only the city.榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "model answer-4: 当然是在城里了。。。益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰\n",
      "\n",
      "\n",
      "\n",
      "prompt: 古代官职升迁又怎么说的，比如：拜。。。最好还有例句！\n",
      "label: 古代官职的任免升迁常用有以下词语：①拜：按一定的授予某种官职或名位；任命。例如柳宗元的《封建论》：“朝拜而不到，夕斥之矣。”②除：任命；拜官。例如《史记·魏其武安侯列传》：“上乃曰：‘君除吏已尽未？吾亦欲除吏。’”③迁：调动官职。一般指提升。例如《后汉书·张衡传》：“拜郎中，再迁为太史令。”迁又为放逐；流放。例如柳宗元的《封建论》：“及夫人逆不道，然后掩捕而迁之。”［左迁］：降职。例如《汉书·周昌传》：“吾极知其左迁。”“左迁”还特指贬官在外。例如刘禹锡《初至长安》诗：“左迁凡二纪，重见帝城春。”［迁客］：被贬到外地的官。例如范仲淹《岳阳楼记》：“迁客骚人，多会于此。”④擢：选拔；提拔。例如《汉书·霍光传》：“擢郎为九江太守。”古代平级调动官职叫“转”或“徙”。例如《后汉书·张衡传》：“顺帝初，再转复为太史令，衡不慕当世，所居之官积年不徙。”⑤黜：废；贬退；罢免或降职。例如柳宗元的《封建论》：“有罪得以黜，有能得以赏。”［黜陟］：（官吏）降级或升等。例如《书·舜典》：“三载考绩，三考黜陟幽明。”⑥谪：贬谪；（官吏）因罪被降职并外放。例如柳宗元的《愚溪诗序》：“余以愚触罪，谪潇水上。”⑦出：（官吏）外放；外调。就是京城的官调到地方任职。例如柳宗元的《哭连州凌员外司马》诗：“出守乌江浒。”⑧乞骸骨。封建社会，大臣年老了请求辞职为“乞骸骨”，意思是请求赐还自己的身体，回到家乡。 \n",
      "model answer-0: 古代官职升迁又怎么说的,比如:拜。。。最好还有例句!官宣   官职升迁                                                               \n",
      "model answer-1:                                                                                   \n",
      "model answer-2: 古代官职升迁又怎么说的,比如:拜。。。最好还有例句!榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "model answer-3: 古代官职升迁又怎么说的,比如:拜。。。最好还有例句!KbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKb\n",
      "model answer-4: 官职升迁又怎么说的,比如:拜。。。最好还有例句!戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛\n",
      "\n",
      "\n",
      "\n",
      "prompt: 在倾斜的离心管中加入和密度小于水的小球，为什么小球会下降？请给出解题步骤。\n",
      "label: 题目应该是这样: 在倾斜的离心管中加入水和密度小于水的小球，在离心管转动时,为什么小球会下降(向转动轴方向运动)？解:在水中设有一个体积与小球相同的水球，这个水球与小球在同一位置（到转动轴的距离相同）上，由于水的密度大于小球的密度，所以水球的质量大于小球的质量，在离心管转动时水球能够保持不动，满足水球受到的合力提供向心力；而这时小球受到的合力与水球相同但是其质量小于水球，其向心力F=mv^2/r,小于水球需要的向心力，所以有小球受到的合力大于向心力，小球将做向心运动，最终运动到距离转动轴最近处。\n",
      "model answer-0: 根据小球的运动状态可知,小球的运动速度和质量成正比,即质量大的小球运动速度越大,小球速度越大,那么小球的质量越大,小球下降的可能性就越小。榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "model answer-1: 离心管内的小球因重力而产生的离心力是使小球静止的,所以小球在离心管内的重力是使小球下降的,所以小球会下降.方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒\n",
      "model answer-2: 重力是小球的重力,小球的重力是水的重力。小球的重力是水的重力。小球的重力是水的重力。タタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタ夏亚タ夏亚夏亚夏亚夏亚夏亚夏亚夏亚<e\n",
      "model answer-3: 由于小球体积较小,在离心力的作用下,小球会做匀速直线运动,小球的运动方向与水的运动方向一致,所以小球会下降.749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749 749749749749749749749749749749749749749749\n",
      "model answer-4: 在倾斜的离心管中加入和密度等于水的小球,为什么小球会下降?                                                                          \n",
      "\n",
      "\n",
      "\n",
      "prompt: 邯郸市哪里有卖纳威沙发 \n",
      "label: 邯郸市亚森家具城内有卖的！ 地址：邯郸市中华大街天客隆超市对过即到！\n",
      "model answer-0: 去邯郸市家具厂有的                                                                                   \n",
      "model answer-1: 我家在邯郸市南环路上,我在邯郸市火车站附近,你可以去看看。榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "model answer-2: 邯郸市东环路的金源时代广场那里有的卖蔺蔺蔺蔺蔺蔺蔺戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛\n",
      "model answer-3: 现在好多的沙发店都有,你可以去看看,或者去他们家看看,他们家的沙发质量都很好,而且价格也很实惠,价格在200元到300元之间,我就是在他们家买的沙发,质量很好,价格在200元到300元之间,你可以去看看,或者去他们家看看,他们家的沙发质量都很好,价格也很实惠,价格在200元到300元之间,你可以去看看,或者去他们家看看,他们家的沙发质量都很好,价格也很实惠,价格在200元到300元之间,你可以去看看,或者去他们家看看,他们家的沙发质量都很好,价格也很实惠,价格在200元到300元之间,你可以去看看,或者去他们家看看,\n",
      "model answer-4: 有啊,你去看看吧蔺蔺蔺蔺蔺蔺戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛\n",
      "\n",
      "\n",
      "\n",
      "prompt: 如图所示，，A、B是圆的直径的两端，小张在A点，小王在B点，同时出发相向而行，他们在点第一次相遇，C点离A点80米；在D点第二次相遇，D点离B点60米。求这个圆的周长。\n",
      "label: 可设方程解答：设圆周长为X米，则得方程，80*2=60+X/2-80X=360（米）（第一次相遇，小张、小王合走半个周长；第二次相遇，小张、小王合走一个周长，即比第一次多走一培路程。这样，小张第一次走路程乘2就等于小张第二次走路程）你的解也是其中方法之一：第一次相遇，小张、小王合走半个周长AB，其中小张走路程AC是80米；第二次相遇，小张、小王合走一个周长 = CD（小王走的左边部分）+CD（小张走的右边部分）。第二次相遇时，小张、小王合走一个周长，等于第一次相遇时，合走半个周长的两培。所以，小张走的右边部分CD就是第一次走的80米（AC）的两培８０＊２（合走与个人走的按比例同时增加两培）。两次相遇时，小张走的路程加起来就是：AD=AC+CD=８０＊２+８０=８０＊３=２４０Ａ到Ｂ（半周长）距离：２４０－６０＝１８０ 即AB=AD-BC\n",
      "model answer-0: 解:设AB于点C, 求证:                                                        \n",
      "model answer-1:                                                               \n",
      "model answer-2: 图象的意思是:A、B两圆直径的两端,小张在A点,小王在B点,同时出发相向而行,他们在点第一次相遇,C点离A点80米;在D点第二次相遇,D点离B点60米。求这个圆的周长。NdFeBNdFeBNdFeBNdFeB\n",
      "model answer-3: C=πd(2*2)=8π=16榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "model answer-4: 解:设圆半径为r,圆心角为α,点A,B为圆的直径,圆心角为β,AB为圆的中点,根据圆的周长公式,得到:                                       \n",
      "\n",
      "\n",
      "\n",
      "prompt: Word文档,一页半文字,怎样调整使其真好布满两页? \n",
      "label: 可以全选后点右键选段落,在里面将字体加大,并且加大行距\n",
      "model answer-0: 1、先将每页的内容复制到记事本,然后点“插入”,在“页眉和页脚”中,将“页码”改为“1”,“页码”下的“间距”改为“1”。2、点击“页面设置”,在“纸张大小”中,将“页码”改为“4”。3、点击“页面设置”,在“纸张大小”中,将“页码”改为“5”。4、点击“页面设置”,在“纸张大小”中,将“页码”改为“7”。5、点击“页面设置”,在“纸张大小”中,将“页码”改为“9”。6、点击“页面设置”,\n",
      "model answer-1: 你好!打开Word后,单击菜单栏“工具”——“选项”——“格式”——“字体”——“横排”——“竖排”,然后将“横排”的前边框选,点“确定”即可。方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒\n",
      "model answer-2: 方法一 :在文档中插入一个空白文档,然后在文本框中输入“1”,点击“插入”,在“文本框”中输入“1”。然后点击“格式”,在“文本框”中输入“1”,点击“确定”。方法二 :在文档中插入一个空白文档,然后在文本框中输入“2”,点击“插入”,在“文本框”中输入“2”。然后点击“格式”,在“文本框”中输入“2”,点击“确定”。 Kbxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "model answer-3: 把页眉页脚都调整成全黑的状态,再到文档中设置页眉页脚即可!戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛\n",
      "model answer-4: 你说的是在Word中,在文档的一页上,写一个“√”,然后,在光标处,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再\n",
      "\n",
      "\n",
      "\n",
      "prompt: 武汉江夏分局接到群众举报电话,说在天桥下有两个大炸弹,江夏分局民警与排弹专家一起奔赴现场,在桥梁下发现一个红色布袋,专家与民警小心翼翼的拆开布袋,里面还有几层报纸包裹,民警一层层拆开,最后发现果然是两个大炸弹:4个2和一对王…… 警察表示要严肃追究报警人责任\n",
      "label: 好一个出大事了，好一个炸弹！又着实被你忽悠一下，还以为出什么大事了。\n",
      "model answer-0:                根据《中华人民共和国治安管理处罚法》第23条,对举报人的治安处罚如下:                           \n",
      "model answer-1:                                                     \n",
      "model answer-2: 这就是武昌的“大炸弹”!                                                \n",
      "model answer-3:                                                     \n",
      "model answer-4:                                                     \n",
      "\n",
      "\n",
      "\n",
      "prompt: 考点知识清单一1．地图上的方向（1）一般情况：面对地图，上_____下_____,左西右东（2）有指向标的地图：指向标的箭头指向_____方（3）有经纬网的地图：经线指示_____，纬线指示_____2．方格状或圆弧形的经纬网图（1）辨别南、北纬纬度数值向北递增为北纬纬度数值向南递增为南纬（2）辨别东、西经经度数值向东递增为_____经经度数值向西递增为_____经3．以南北极为中心的经纬网图（1）辨别南北图上标出的地球自转方向呈逆时针，则经纬网中心为_____极图上标出的地球自转方向考点知识清单一1．地图上的方向（1）一般情况：面对地图，上_____下_____,左西右东（2）有指向标的地图：指向标的箭头指向_____方（3）有经纬网的地图：经线指示_____，纬线指示_____2．方格状或圆弧形的经纬网图（1）辨别南、北纬纬度数值向北递增为北纬纬度数值向南递增为南纬（2）辨别东、西经经度数值向东递增为_____经经度数值向西递增为_____经3．以南北极为中心的经纬网图（1）辨别南北图上标出的地球自转方向呈逆时针，则经纬网中心为_____极图上标出的地球自转方向呈顺时针，则经纬网中心为_____极（2）辨别东西根据地球自转方向自西向东，即自转箭头指东东经度数向东增大，向西减小，西经反之4． 地图上的比例尺（1）概念：表示图上距离比实地距离缩小的程度，比例尺=_____距离/实地距离（2）表示形式：数字式_______________、文字式_______________、线段式_______________（3）用途：确定图上两个地点的实际距离（4）大小比较比例尺的分母愈大，比例尺愈_____，表示的范围愈_____，内容愈粗略比例尺的分母愈小，比例尺愈_____，表示的范围愈_____，内容愈详细5．地图上的高低起伏（1）高度表示：_____高度（海拔），其定义_______________、相对高度，其定义_______________（2）地形判读坡度陡缓等高线密集处，坡度越_____等高线稀疏处，坡度越_____地形种类山顶，_________________________盆地，_________________________鞍部，_________________________山脊，_________________________山谷，_________________________\n",
      "label: 考点知识清单一地图1．地图上的方向（1）一般情况：面对地图，上（北）下（南）,左西右东（2）有指向标的地图：指向标的箭头指向（北）方（3）有经纬网的地图：经线指示（南北方向），纬线指示（东西方向）2．方格状或圆弧形的经纬网图（1）辨别南、北纬纬度数值向北递增为北纬纬度数值向南递增为南纬（2）辨别东、西经经度数值向东递增为（东）经经度数值向西递增为（西）经3．以南北极为中心的经纬网图（1）辨别南北图上标出的地球自转方向呈逆时针，则经纬网中心为（北）极图上标出的地球自转方向呈顺时针，则经纬网中心为（南）极（2）辨别东西根据地球自转方向自西向东，即自转箭头指东东经度数向东增大，向西减小，西经反之4． 地图上的比例尺（1）概念：表示图上距离比实地距离缩小的程度，比例尺=图上距离/实地距离（2）表示形式：数字式（1/100)、文字式(1厘米代表100千米）、线段式（不太好画）（3）用途：确定图上两个地点的实际距离（4）大小比较比例尺的分母愈大，比例尺愈（小），表示的范围愈（大），内容愈粗略比例尺的分母愈小，比例尺愈（大），表示的范围愈（小），内容愈详细5．地图上的高低起伏（1）高度表示：（地面）高度（海拔），其定义（地面某个地点高出海平面的垂直距离）、相对高度，其定义（某个地点高出另一地点的垂直距离）（2）地形判读坡度陡缓等高线密集处，坡度越（陡）等高线稀疏处，坡度越（缓）地形种类---------题目意思不懂？山顶，_________________________盆地，_________________________鞍部，_________________________山脊，山谷，不会做啦。初一第一册书有呀。。。再找找呼。。。。。。－ －！\n",
      "model answer-0: 1\n",
      "model answer-1: 1\n",
      "model answer-2: 1\n",
      "model answer-3: 1\n",
      "model answer-4: 1\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_return_sequences = 5\n",
    "i = 10\n",
    "j = 20\n",
    "t1 = time.time()\n",
    "results = text_generator(prompts_processed[i:j], max_length=200, num_return_sequences=num_return_sequences,\n",
    "                         do_sample=True, top_k=50, temperature=10.0)\n",
    "print(f\"Finished prediction, time taken: {time.time()-t1}\")\n",
    "\n",
    "for prompt, res, label in zip(prompts[i:j], results[:(j-i)], labels[i:j]):\n",
    "    print(f\"prompt: {prompt}\\nlabel: {label}\")\n",
    "    for k in range(num_return_sequences):\n",
    "        model_answer = res[k]['generated_text'].split(\"模型回答:\")[1].replace(\"<eot>\", \"\").replace(\"<pad>\", \"\")\n",
    "        print(f\"model answer-{k}: {model_answer}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: 内有吊车，牛腿高度怎么定？根据什么来定？生产的工艺要求？还是要考虑到其他因素？\n",
      "model answer-0: 应该没有规定棍子高度标准.可以按以下几点.⁇底高度一般是在5米~10,1M;长度要长1-5;宽度是要=50cmx2‰;用到2支牛手:50、±@+50,<E8%]50:4,0x20^40*80;30、100、200*60^60;400X200、60X60x400(按2/2.5或2.5/4折算成4/6则需要5米长牛棒,长4倍多就行了>>长度、用@:4-x0-60,X50x50*40.根据不同牛体选用量。用4-50-\n",
      "model answer-1: 用“体型和质量尺寸比例关系方准的尺寸精度”计算出所规定范围和长度尺寸;在此中应确定每段高度是根据“零件加工质量几何图形表示、轮廓线”而进行绘制的:体型即加工质量(T1)图2某产品零件加工后工序顺序图 工位表:工号:生产周期/月或工作量Tmax工作频/H/每根线每道每排加工的总小时工时数Sum:(长度或尺寸的量化位数(如“1#-10/4);3’+10%+1]≥40°+25m/3’√H2,2,1+40%’为(长,(\n",
      "model answer-2: 生产用什么,比如汽车发动机还是什么整机什么,在用也罢(包括生产配件也就是模具〕要考虑什么等等要在图纸中有交代你!具体你在厂牌里应该了解具体一点的'具体就多些的啊~)希望对题!希望会有助!你就在这里留言你也看!如果帮到点了记得留下宝贵的经验!谢字在我评论上.还有一点你如果想在淘宝里开卖点精致或者你不嫌脏,还是去买个牌子货.不就知道买哪个比较的?其实也不要急着做大了 如果有问题也会有个问题找商家给你答复哦如果有的没的的买还是建议直接买,不急是吗\n",
      "model answer-3: 一般说来长度应该和牛肩高和长度对应成1.15M或者1.26MM为适合吊钩'子牛吊杆高度为1.8,2m的高度.你要买什么样大小才方便,可以买5'8MM或者10～15的规格尺寸吊在车钩梢的一头和钩的两边即可根据你是哪种规格你吊多少高度的产品一般要考虑下面三块问题.首先:如果吊机要保证在任何的作业场所都可顺利操作(如在高处起料时就应该是高度超过牛的高度3米才对)其次如果要对货要保持足够空间的高度最后根据市场情况确定货的多少你选择挂的时候注意下挂钩是对勾 如果不\n",
      "model answer-4: 要具体的看,有几个因素你自己去问车间主管,也要看产品的规格说明或其他资料。看多,对了会对工作的改进带来很大促进;你问他,一般他们对生产过程有什么细节有要求。你要自己研究去确定自己工作要做好到哪一点才有提高生产量及生产效果=我有这样方法给生产人员提一些:如果有条件一定先在仓库配种后,有时间可以带工人自己定吊力粗细;(如果要吊的粗点再根据自己实际做出来吊高。这和工人师傅一起探讨)你生产出一批有效果才有发展。我看现在一般配种做不到这点啊要是再吊一下面积的配比再\n"
     ]
    }
   ],
   "source": [
    "prompt = \"内有吊车，牛腿高度怎么定？根据什么来定？生产的工艺要求？还是要考虑到其他因素？\"\n",
    "prompt_processed = prompt + tokenizer.sep_token + \"模型回答:\"\n",
    "num_return_sequences=5\n",
    "res = text_generator(prompt_processed, max_length=200, num_return_sequences=num_return_sequences,\n",
    "                         do_sample=True, top_p=0.5, temperature=10.0)\n",
    "print(f\"prompt: {prompt}\")\n",
    "for i in range(num_return_sequences):\n",
    "    model_answer = res[i]['generated_text'].split(\"模型回答:\")[1].replace(\"<eot>\", \"\").replace(\"<pad>\", \"\")\n",
    "    # print(res)\n",
    "    print(f\"model answer-{i}: {model_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = \"D:\\\\Data\\\\models\\\\pangu-350M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_cache=False, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'unk_token': \"<unk>\",\n",
    "                                  'bos_token': \"<s>\",\n",
    "                                  'eos_token': \"<eot>\",\n",
    "                                  'pad_token': \"<pad>\",\n",
    "                                  \"sep_token\": \"<sep>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\SUNZEY~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.501 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "max_length = 1024\n",
    "text = \"你好，你是谁\"\n",
    "# text = \"<|startoftext|>\" + text + \"<|endoftext|>\"\n",
    "res = tokenizer(text, max_length=max_length, truncation=\"longest_first\", \n",
    "          return_tensors=\"pt\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()\n",
    "# torch.cat((res['input_ids'], res['input_ids']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
