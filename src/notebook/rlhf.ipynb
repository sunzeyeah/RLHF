{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0222 16:02:19.297271 140704479725184 redirects.py:28] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import os, time, re, random, glob, json, jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    TextGenerationPipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device=\"cuda:0\"\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_TEXT_PATTERN = re.compile(r\"[\\r\\n]\")\n",
    "\n",
    "def clean_text(text):\n",
    "    return CLEAN_TEXT_PATTERN.sub(\"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece\n",
    "model_file = \"/Users/zeyesun/Documents/Data/models/pangu-350M/vocab.model\"\n",
    "sp = sentencepiece.SentencePieceProcessor()\n",
    "sp.Load(model_file=model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<s>\n",
      "</s>\n",
      "▃\n",
      "▂\n",
      "<sep>\n",
      "<pad>\n",
      "<mask>\n",
      "<eod>\n",
      "<eot>\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(sp.id_to_piece(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_name_or_path = \"D:\\\\Data\\\\models\\\\pangu_2_6B\"\n",
    "# model_name_or_path = \"D:\\\\Data\\\\models\\\\pangu-350M\"\n",
    "model_name_or_path = \"/Users/zeyesun/Documents/Data/models/pangu-350M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'eos_token': \"<eot>\", 'pad_token': \"<pad>\", \"sep_token\": \"<sep>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of GPTPanguForCausalLM were not initialized from the model checkpoint at /Users/zeyesun/Documents/Data/models/pangu-350M and are newly initialized: ['transformer.h.9.attn.bias', 'transformer.h.0.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.20.attn.bias', 'transformer.h.19.attn.bias', 'transformer.h.22.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.7.attn.masked_bias', 'lm_head.weight', 'transformer.h.6.attn.bias', 'transformer.h.16.attn.bias', 'transformer.h.13.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.18.attn.bias', 'transformer.h.5.attn.bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.17.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.11.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.19.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTPanguForCausalLM(\n",
       "  (transformer): GPTPanguModel(\n",
       "    (wte): Embedding(40000, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (wqe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=40000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, use_cache=False)\n",
    "model.resize_token_embeddings(len(tokenizer.sp))\n",
    "model.config.end_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.max_length_prompt = 200\n",
    "model.to(device)\n",
    "# print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1425170\n"
     ]
    }
   ],
   "source": [
    "# f = \"/Users/zeyesun/Documents/Data/raw/baike_qa2019/baike_qa_train.json\"\n",
    "f = \"D:\\\\Data\\\\raw\\\\baike_qa_train.json\"\n",
    "items = []\n",
    "lens_prompt = []\n",
    "lens_label = []\n",
    "with open(f, \"r\", encoding=\"utf-8\") as r:\n",
    "    while True:\n",
    "        line = r.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        item = json.loads(line.strip(\"\\n\"))\n",
    "        prompt = clean_text(item['title'] if len(item['title']) > len(item['desc']) else item['desc'])\n",
    "        label = clean_text(item['answer'])\n",
    "        items.append(item)\n",
    "        lens_prompt.append(len(prompt))\n",
    "        lens_label.append(len(label))\n",
    "print(len(items))\n",
    "print(np.percentile(lens_prompt, np.arange(90, 101)))\n",
    "print(np.percentile(lens_label, np.arange(90, 101)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 'qid_7770627748113178417',\n",
       " 'category': '生活-保健养生',\n",
       " 'title': '减肥健身我要下载一套很好的有氧健身操，请问哪里有下载地址，谢谢！ ',\n",
       " 'desc': '我要下载一套很好的有氧操，请问哪里有下载地址，谢谢！',\n",
       " 'answer': '我看练太极拳就可以，该运动也是有氧健身的运动，长期锻炼还可以治疗很多慢性病。'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 789\n",
    "items[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我看练太极拳就可以，该运动也是有氧健身的运动，长期锻炼还可以治疗很多慢性病。'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[i]['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"/Users/zeyesun/Documents/Data/output/sft/pangu-350M/checkpoint-12000/pytorch_model.bin\"\n",
    "st = torch.load(checkpoint, map_location=\"cpu\")\n",
    "model.load_state_dict(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'GPTPanguForCausalLM' is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GPT2LMHeadModel', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel'].\n"
     ]
    }
   ],
   "source": [
    "text_generator = TextGenerationPipeline(model, tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"/Users/zeyesun/Documents/Data/raw/baike_qa2019/baike_qa_train.json\"\n",
    "i = 0\n",
    "prompts = []\n",
    "prompts_processed = []\n",
    "labels = []\n",
    "with open(f, \"r\", encoding=\"utf-8\") as r:\n",
    "    while True:\n",
    "        line = r.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        item = json.loads(line.strip(\"\\n\"))\n",
    "        prompt = clean_text(item['title'] if len(item['title']) > len(item['desc']) else item['desc'])\n",
    "        label = clean_text(item['answer'])\n",
    "        prompt_processed = prompt + tokenizer.sep_token + \"模型回答:\"\n",
    "        prompts.append(prompt)\n",
    "        prompts_processed.append(prompt_processed)\n",
    "        labels.append(label)\n",
    "        i += 1\n",
    "        if i > 1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 655, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished prediction, time taken: 318.54285526275635\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "j = 20\n",
    "t1 = time.time()\n",
    "results = text_generator(prompts_processed[i:j], max_length=100)\n",
    "print(f\"Finished prediction, time taken: {time.time()-t1}\")\n",
    "for prompt, res, label in zip(prompts[i:j], results[i:j], labels[i:j]):\n",
    "    model_answer = res[0]['generated_text'].split(\"模型回答:\")[1].replace(\"<eot>\", \"\")\n",
    "    print(f\"prompt: {prompt}\\nmodel answer: {model_answer}\\nlabel: {label}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['有6区无尽之海众神之子工会的吗?你们会怎么了?看不见人了,解散了 <sep>模型回答:',\n",
       " '老师好，青青稞酒明天再跌就可以建仓了吗？谢谢。 <sep>模型回答:',\n",
       " 'onthesouthofthecity是在城内还是成外？ <sep>模型回答:',\n",
       " '古代官职升迁又怎么说的，比如：拜。。。最好还有例句！<sep>模型回答:',\n",
       " '在倾斜的离心管中加入和密度小于水的小球，为什么小球会下降？请给出解题步骤。<sep>模型回答:',\n",
       " '邯郸市哪里有卖纳威沙发 <sep>模型回答:',\n",
       " '如图所示，，A、B是圆的直径的两端，小张在A点，小王在B点，同时出发相向而行，他们在点第一次相遇，C点离A点80米；在D点第二次相遇，D点离B点60米。求这个圆的周长。<sep>模型回答:',\n",
       " 'Word文档,一页半文字,怎样调整使其真好布满两页? <sep>模型回答:',\n",
       " '武汉江夏分局接到群众举报电话,说在天桥下有两个大炸弹,江夏分局民警与排弹专家一起奔赴现场,在桥梁下发现一个红色布袋,专家与民警小心翼翼的拆开布袋,里面还有几层报纸包裹,民警一层层拆开,最后发现果然是两个大炸弹:4个2和一对王…… 警察表示要严肃追究报警人责任<sep>模型回答:',\n",
       " '考点知识清单一1．地图上的方向（1）一般情况：面对地图，上_____下_____,左西右东（2）有指向标的地图：指向标的箭头指向_____方（3）有经纬网的地图：经线指示_____，纬线指示_____2．方格状或圆弧形的经纬网图（1）辨别南、北纬纬度数值向北递增为北纬纬度数值向南递增为南纬（2）辨别东、西经经度数值向东递增为_____经经度数值向西递增为_____经3．以南北极为中心的经纬网图（1）辨别南北图上标出的地球自转方向呈逆时针，则经纬网中心为_____极图上标出的地球自转方向考点知识清单一1．地图上的方向（1）一般情况：面对地图，上_____下_____,左西右东（2）有指向标的地图：指向标的箭头指向_____方（3）有经纬网的地图：经线指示_____，纬线指示_____2．方格状或圆弧形的经纬网图（1）辨别南、北纬纬度数值向北递增为北纬纬度数值向南递增为南纬（2）辨别东、西经经度数值向东递增为_____经经度数值向西递增为_____经3．以南北极为中心的经纬网图（1）辨别南北图上标出的地球自转方向呈逆时针，则经纬网中心为_____极图上标出的地球自转方向呈顺时针，则经纬网中心为_____极（2）辨别东西根据地球自转方向自西向东，即自转箭头指东东经度数向东增大，向西减小，西经反之4． 地图上的比例尺（1）概念：表示图上距离比实地距离缩小的程度，比例尺=_____距离/实地距离（2）表示形式：数字式_______________、文字式_______________、线段式_______________（3）用途：确定图上两个地点的实际距离（4）大小比较比例尺的分母愈大，比例尺愈_____，表示的范围愈_____，内容愈粗略比例尺的分母愈小，比例尺愈_____，表示的范围愈_____，内容愈详细5．地图上的高低起伏（1）高度表示：_____高度（海拔），其定义_______________、相对高度，其定义_______________（2）地形判读坡度陡缓等高线密集处，坡度越_____等高线稀疏处，坡度越_____地形种类山顶，_________________________盆地，_________________________鞍部，_________________________山脊，_________________________山谷，_________________________<sep>模型回答:']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_processed[i:j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "I0222 16:03:10.048795 140704479725184 __init__.py:113] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/wf/12stcn_s6zq56j9h3fnkv5lm0000gn/T/jieba.cache\n",
      "I0222 16:03:10.050362 140704479725184 __init__.py:133] Loading model from cache /var/folders/wf/12stcn_s6zq56j9h3fnkv5lm0000gn/T/jieba.cache\n",
      "Loading model cost 0.572 seconds.\n",
      "I0222 16:03:10.622334 140704479725184 __init__.py:165] Loading model cost 0.572 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "I0222 16:03:10.623375 140704479725184 __init__.py:166] Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '内有吊车，牛腿高度怎么定？根据什么来定？生产的工艺要求？还是要考虑到其他因素？<sep>模型回答:牛腿高度是指牛腿的长度,一般是指牛腿'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"内有吊车，牛腿高度怎么定？根据什么来定？生产的工艺要求？还是要考虑到其他因素？\"\n",
    "prompt_processed = prompt + tokenizer.sep_token + \"模型回答:\"\n",
    "res = text_generator(prompt_processed, max_length=50)\n",
    "model_answer = res[0]['generated_text'].split(\"模型回答:\")[1].replace(\"<eot>\", \"\")\n",
    "# print(res)\n",
    "print(f\"prompt: {prompt}\\nmodel answer: {model_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = \"D:\\\\Data\\\\models\\\\pangu-350M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_cache=False, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'unk_token': \"<unk>\",\n",
    "                                  'bos_token': \"<s>\",\n",
    "                                  'eos_token': \"<eot>\",\n",
    "                                  'pad_token': \"<pad>\",\n",
    "                                  \"sep_token\": \"<sep>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\SUNZEY~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.501 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "max_length = 1024\n",
    "text = \"你好，你是谁\"\n",
    "# text = \"<|startoftext|>\" + text + \"<|endoftext|>\"\n",
    "res = tokenizer(text, max_length=max_length, truncation=\"longest_first\", \n",
    "          return_tensors=\"pt\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()\n",
    "# torch.cat((res['input_ids'], res['input_ids']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
