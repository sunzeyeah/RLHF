{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, re, random, glob, json, jieba, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    TextGenerationPipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "from sys import platform\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    # linux\n",
    "    root = \"/mnt/private-pa002-vol141056-prd/Data\"\n",
    "elif platform == \"darwin\":\n",
    "    # OS X\n",
    "    root = \"/Users/zeyesun/Documents/Data\"\n",
    "elif platform == \"win32\":\n",
    "    # Windows...\n",
    "    root = \"D:\\\\Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_TEXT_PATTERN = re.compile(r\"[\\r\\n]\")\n",
    "\n",
    "def clean_text(text):\n",
    "    return CLEAN_TEXT_PATTERN.sub(\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentencepiece\n",
    "# model_file = os.path.join(root, \"models\", \"pangu-350M\", \"vocab.model\")\n",
    "# sp = sentencepiece.SentencePieceProcessor()\n",
    "# sp.Load(model_file=model_file)\n",
    "# for i in range(10):\n",
    "#     print(sp.id_to_piece(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = os.path.join(root, \"models\", \"pangu-350M\")\n",
    "# model_name_or_path = os.path.join(root, \"Data\", \"models\", \"pangu-2.6B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({\n",
    "    'unk_token': \"<unk>\", \n",
    "    'eos_token': \"<eot>\", \n",
    "    'pad_token': \"<pad>\", \n",
    "    \"sep_token\": \"<sep>\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weibo_summary_comments_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 894732, time taken: 68.99229574203491 s\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "fi = os.path.join(root, \"raw\", \"weibo_summary_comments_json.json\")\n",
    "fo = os.path.join(root, \"chatgpt\", \"processed\", \"weibo_summary_comments.jsonl\")\n",
    "ct = 0\n",
    "with open(fo, \"w\", encoding=\"utf-8\") as w:\n",
    "    with open(fi, \"r\", encoding=\"utf-8\") as r:\n",
    "        while True:\n",
    "            line = r.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            \n",
    "            item = json.loads(line.strip(\"\\n\"))\n",
    "            article = item['article'].replace(\" \", \"\")\n",
    "            abstract = item['abstract'].replace(\" \", \"\")\n",
    "            prompt = f\"新闻内容：{article}{tokenizer.sep_token}摘要：{abstract}{tokenizer.sep_token}评论：\"\n",
    "            answers = [\n",
    "                {\n",
    "                    \"answer\": k.replace(\" \", \"\"), \n",
    "                    \"score\": int(v)\n",
    "                } for (k, v) in sorted(item['comments'], key=lambda x: (int(x[1]), len(x[0])), reverse=True)\n",
    "            ]\n",
    "            w.write(json.dumps({\"prompt\": prompt, \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "            ct += 1\n",
    "print(f\"length: {ct}, time taken: {time.time()-t} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### couplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 774491, # different lengths: 32, time taken: 3.667919635772705 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Couplets: 0it [00:00, ?it/s]C:\\Users\\sunzeyeah\\AppData\\Local\\Temp\\ipykernel_5568\\1857859562.py:32: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  false_answers_1 = [{\"answer\": fa, \"score\": 0} for fa in random.sample(nexts_tmp, 2)]\n",
      "C:\\Users\\sunzeyeah\\AppData\\Local\\Temp\\ipykernel_5568\\1857859562.py:36: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  false_answers_2 = [{\"answer\": random.choice(nexts[key]), \"score\": -1} for key in random.sample(keys, 2)]\n",
      "Processing Couplets: 986it [00:57, 12.84it/s]"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "fi = os.path.join(root, \"raw\", \"couplets.txt\")\n",
    "fo = os.path.join(root, \"chatgpt\", \"processed\", \"couplets.jsonl\")\n",
    "l2 = []\n",
    "nexts = dict()\n",
    "with open(fi, \"r\", encoding=\"utf-8\") as r:\n",
    "    while True:\n",
    "        line = r.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.strip(\"\\n\")\n",
    "        idx = len(line) // 2\n",
    "        prompt = line[:idx]\n",
    "        answer = line[idx+1:]\n",
    "        answers = [{\"answer\": answer, \"score\": 1}]\n",
    "        l2.append({\"prompt\": f\"上联：{prompt}{tokenizer.sep_token}下联：\", \"answers\": answers})\n",
    "        length = len(answer)\n",
    "        if length not in nexts:\n",
    "            nexts[length] = list()\n",
    "        nexts[length].append(answer)\n",
    "t2 = time.time()\n",
    "print(f\"length: {len(l2)}, # different lengths: {len(nexts)}, time taken: {t2-t1} s\")\n",
    "with open(fo, \"w\", encoding=\"utf-8\") as w:\n",
    "    for i, l in tqdm(enumerate(l2), desc=\"Processing Couplets\"):\n",
    "        answer = l['answers'][0]\n",
    "        length = len(answer['answer'])\n",
    "        # 上下联长度一样\n",
    "        nexts_tmp = set(nexts[length])\n",
    "        nexts_tmp.remove(answer['answer'])\n",
    "        nexts_tmp = set(nexts[length]).difference(set([answer['answer']]))\n",
    "#         nexts_tmp.remove(answer['answer'])\n",
    "        false_answers_1 = [{\"answer\": fa, \"score\": 0} for fa in random.sample(nexts_tmp, 2)]\n",
    "        # 上下联长度不一样\n",
    "        keys = set(nexts.keys())\n",
    "        keys.remove(length)\n",
    "        false_answers_2 = [{\"answer\": random.choice(nexts[key]), \"score\": -1} for key in random.sample(keys, 2)]\n",
    "        answers = [answer] + false_answers_1 + false_answers_2\n",
    "#         answers = sorted(answers, key=lambda x: x['score'], reverse=True)\n",
    "        w.write(json.dumps({\"prompt\": l['prompt'], \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "#         if i % 1000 == 0:\n",
    "#             print(f\"{i} samples processed, time taken: {time.time()-t2} s\")\n",
    "print(f\"length: {len(l2)}, time taken: {time.time()-t2} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zhidao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing financezhidao_filter.csv\n",
      "finished processing liantongzhidao_filter.csv\n",
      "finished processing touzizhidao_filter.csv\n",
      "finished processing nonghangzhidao_filter.csv\n",
      "finished processing baoxianzhidao_filter.csv\n",
      "finished processing anhuidianxinzhidao_filter.csv\n",
      "finished processing lawzhidao_filter.csv\n",
      "length: 36368, time taken: 127.75226378440857 s\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "fp = os.path.join(root, \"raw\", \"zhidao\", \"*.csv\")\n",
    "fo = os.path.join(root, \"chatgpt\", \"processed\", \"zhidao.jsonl\")\n",
    "ct = 0\n",
    "with open(fo, \"w\", encoding=\"utf-8\") as w:\n",
    "    for fi in glob.glob(fp):\n",
    "        ct = 0\n",
    "        df = pd.read_csv(fi).sort_values(by=[\"title\", \"is_best\"], ascending=False)\n",
    "        prev_title = None\n",
    "        prev_prompt = None\n",
    "        for _, val in df.iterrows():\n",
    "            if isinstance(val['question'], str) and val['question'] != val['title']:\n",
    "                prompt = f\"问题：{val['title']}{tokenizer.sep_token}内容：{val['question']}{tokenizer.sep_token}回答：\"\n",
    "            else:\n",
    "                prompt = f\"问题：{val['title']}{tokenizer.sep_token}回答：\"\n",
    "            if prev_title is not None and prev_title == val['title']:\n",
    "                answers.append({\"answer\": val['reply'], \"score\": val['is_best']})\n",
    "            else:\n",
    "                if prev_title is not None:\n",
    "#                     l3.append({\"prompt\": prev_prompt, \"answers\": copy.deepcopy(answers)})\n",
    "                    w.write(json.dumps({\"prompt\": prev_prompt, \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "                answers = [{\"answer\": val['reply'], \"score\": val['is_best']}]\n",
    "            prev_prompt = prompt\n",
    "            prev_title = val['title']\n",
    "            ct += 1\n",
    "#         l3.append({\"prompt\": prev_prompt, \"answers\": copy.deepcopy(answers)})\n",
    "        w.write(json.dumps({\"prompt\": prev_prompt, \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "        print(f\"finished processing {os.path.basename(fi)}\")\n",
    "print(f\"length: {ct}, time taken: {time.time()-t} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JDData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing financezhidao_filter.csv\n",
      "finished processing liantongzhidao_filter.csv\n",
      "finished processing touzizhidao_filter.csv\n",
      "finished processing nonghangzhidao_filter.csv\n",
      "finished processing baoxianzhidao_filter.csv\n",
      "finished processing anhuidianxinzhidao_filter.csv\n",
      "finished processing lawzhidao_filter.csv\n",
      "length: 36368, time taken: 127.75226378440857 s\n"
     ]
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "       #Initializing lists\n",
    "        self.start_tags = list()\n",
    "        self.end_tags = list()\n",
    "        self.start_end_tags = list()\n",
    "        self.data_list = list()\n",
    "    #HTML Parser Methods\n",
    "    def handle_starttag(self, startTag, attrs):\n",
    "        self.start_tags.append(startTag)\n",
    "    def handle_endtag(self, endTag):\n",
    "        self.end_tags.append(endTag)\n",
    "    def handle_startendtag(self,startendTag, attrs):\n",
    "        self.start_end_tags.append(startendTag)\n",
    "    def handle_data(self, data):\n",
    "        self.data_list.append(data)\n",
    "        \n",
    "t = time.time()\n",
    "fi = os.path.join(root, \"raw\", \"JDData\", \"*.data*\")\n",
    "# fo = os.path.join(root, \"chatgpt\", \"processed\", \"zhidao.jsonl\")\n",
    "ct = 0\n",
    "with open(fo, \"w\", encoding=\"utf-8\") as w:\n",
    "    for fi in glob.glob(fp):\n",
    "        ct = 0\n",
    "        with open(fi, \"r\", encoding=\"gbk\") as r:\n",
    "            line = r.readline()\n",
    "            items = line.strip(\"\\n\").split(\"\\t\")\n",
    "            parser = MyHTMLParser()\n",
    "            parser.feed(items[1])\n",
    "            for t, d in zip(parser.start_tags, parser.data_list):\n",
    "                print(f\"{t}: {d}\")\n",
    "#                 prompt = f\"问题：{val['title']}{tokenizer.sep_token}内容：{val['question']}{tokenizer.sep_token}回答：\"\n",
    "#                 answers.append({\"answer\": val['reply'], \"score\": val['is_best']})\n",
    "            ct += 1\n",
    "#         l3.append({\"prompt\": prev_prompt, \"answers\": copy.deepcopy(answers)})\n",
    "#         w.write(json.dumps({\"prompt\": prev_prompt, \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "        print(f\"finished processing {os.path.basename(fi)}\")\n",
    "print(f\"length: {ct}, time taken: {time.time()-t} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yf_amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "fi = os.path.join(root, \"raw\", \"yf_amazon\", \"products.csv\")\n",
    "dfp = pd.read_csv(fi)\n",
    "fi = os.path.join(root, \"raw\", \"yf_amazon\", \"ratings.csv\")\n",
    "dfr = pd.read_csv(fi)\n",
    "fi = os.path.join(root, \"raw\", \"yf_amazon\", \"categories.csv\")\n",
    "dfc = pd.read_csv(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['productId', 'name', 'catIds', 'cate_id_1'], dtype='object')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp.columns\n",
    "# dfp['name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832    图书音像\n",
      "Name: category, dtype: object\n",
      "911    母婴/玩具\n",
      "Name: category, dtype: object\n",
      "1111    运动户外\n",
      "Name: category, dtype: object\n",
      "486    钟表/首饰/眼镜/礼品\n",
      "Name: category, dtype: object\n",
      "1057    电脑/办公\n",
      "Name: category, dtype: object\n",
      "571    家具/家装/建材\n",
      "Name: category, dtype: object\n",
      "933    家居生活\n",
      "Name: category, dtype: object\n",
      "67    食品/保健\n",
      "Name: category, dtype: object\n",
      "57    其他\n",
      "Name: category, dtype: object\n",
      "1128    手机/数码\n",
      "Name: category, dtype: object\n",
      "916    美妆个护\n",
      "Name: category, dtype: object\n",
      "222    家用电器\n",
      "Name: category, dtype: object\n",
      "802    服饰服装\n",
      "Name: category, dtype: object\n",
      "518    鞋类箱包\n",
      "Name: category, dtype: object\n",
      "539    机票/充值/票务/虚拟\n",
      "Name: category, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dfp['cate_id_1'] = dfp['catIds'].apply(lambda x: x.split(\",\")[0])\n",
    "for cid1 in dfp['cate_id_1'].unique():\n",
    "    print(dfc[dfc['catId']==int(cid1)]['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dmsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 3)\n",
      "(2125056, 6)\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "fi = os.path.join(root, \"raw\", \"dmsc\", \"movies.csv\")\n",
    "dfm = pd.read_csv(fi)\n",
    "print(dfm.shape)\n",
    "fi = os.path.join(root, \"raw\", \"dmsc\", \"ratings.csv\")\n",
    "dfr = pd.read_csv(fi)\n",
    "print(dfr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieId\n",
       "0      54153\n",
       "1      83692\n",
       "2      64410\n",
       "3      46233\n",
       "4      44366\n",
       "5     133393\n",
       "6      30475\n",
       "7     109162\n",
       "8      23739\n",
       "9      79962\n",
       "10     91452\n",
       "11     96620\n",
       "12     85677\n",
       "13     26797\n",
       "14     35093\n",
       "15     68359\n",
       "16     78281\n",
       "17    120200\n",
       "18    113687\n",
       "19     83173\n",
       "20     39802\n",
       "21     73882\n",
       "22     88903\n",
       "23     41152\n",
       "24    102876\n",
       "25     58746\n",
       "26    113260\n",
       "27    137511\n",
       "Name: comment, dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfr.groupby(\"movieId\", 'rating').count()['comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese Classical-Modern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 967255, # different lengths: 27, time taken: 6.60289192199707 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chinese Classical-Modern: 0it [00:00, ?it/s]C:\\Users\\sunzeyeah\\AppData\\Local\\Temp\\ipykernel_19380\\1601877548.py:42: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  false_answers_2 = [{\"answer\": random.choice(dicts[key][answer_type]), \"score\": -1} for key in random.sample(keys, 2)]\n",
      "Processing Chinese Classical-Modern: 967255it [19:13:59, 13.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 967254, time taken: 69959.00833964348 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "fp = os.path.join(root, \"raw\", \"Classical-Modern\", \"bitext\", \"*\")\n",
    "fo = os.path.join(root, \"chatgpt\", \"processed\", \"chinese_classical.jsonl\")\n",
    "l3 = []\n",
    "dicts = dict()\n",
    "for fi in glob.glob(fp):\n",
    "    name = os.path.basename(fi)\n",
    "    dicts[name] = {\"古文\": [], \"现代文\": []}\n",
    "    with open(fi, \"r\", encoding=\"utf-8\") as r:\n",
    "        for i, line in enumerate(r):\n",
    "            line = line.strip(\"\\n\")\n",
    "            if line.startswith(\"古文\"):\n",
    "                p1 = line[3:]\n",
    "                dicts[name]['古文'].append(p1)\n",
    "            elif line.startswith(\"现代文\"):\n",
    "                p2 = line[4:]\n",
    "                dicts[name]['现代文'].append(p2)\n",
    "            elif p1 is not None and p2 is not None:\n",
    "                pair = [(\"古文\", p1), (\"现代文\", p2)]\n",
    "                random.shuffle(pair)\n",
    "                prompt = f\"{pair[0][0]}：{pair[0][1]}{tokenizer.sep_token}{pair[1][0]}：\"\n",
    "                answers = [{\"answer\": pair[1][1], \"score\": 1}]\n",
    "                l3.append({\"prompt\": prompt, \"answers\": answers, \"name\": name})\n",
    "                p1 = None\n",
    "                p2 = None\n",
    "t2 = time.time()\n",
    "print(f\"length: {len(l3)}, # different names: {len(dicts)}, time taken: {t2-t1} s\")\n",
    "with open(fo, \"w\", encoding=\"utf-8\") as w:\n",
    "    for i, l in tqdm(enumerate(l3), desc=\"Processing Chinese Classical-Modern\"):\n",
    "        name = l['name']\n",
    "        prompt = l['prompt']\n",
    "        answer = l['answers'][0]['answer']\n",
    "        if prompt.startswith(\"古文\"):\n",
    "            answer_type = '现代文'\n",
    "        else:\n",
    "            answer_type = '古文'\n",
    "        samples_tmp = set(dicts[name][answer_type])\n",
    "        samples_tmp.remove(answer)\n",
    "        false_answers_1 = [{\"answer\": fa, \"score\": 0} for fa in random.sample(samples_tmp, 2)]\n",
    "        keys = set(dicts.keys())\n",
    "        keys.remove(name)\n",
    "        false_answers_2 = [{\"answer\": random.choice(dicts[key][answer_type]), \"score\": -1} for key in random.sample(keys, 2)]\n",
    "        answers = [answer] + false_answers_1 + false_answers_2\n",
    "        w.write(json.dumps({\"prompt\": prompt, \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "#         if i % 100 == 0:\n",
    "#             print(f\"{i} samples processed, time taken: {time.time()-t2} s\")\n",
    "print(f\"length: {i}, time taken: {time.time()-t} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese Poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 345170, # different lengths: 7, time taken: 21.88776707649231 s\n"
     ]
    }
   ],
   "source": [
    "import opencc\n",
    "converter = opencc.OpenCC('t2s.json')\n",
    "t1 = time.time()\n",
    "fp = [\n",
    "    # 四书五经\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"lunyu\", \"lunyu.json\"),\n",
    "#     os.path.join(root, \"raw\", \"chinese-poetry\", \"mengxue\", \"*.json\"),\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"sishuwujing\", \"*.json\"),\n",
    "    # 古体诗\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"caocaoshiji\", \"caocao.json\"),\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"shijing\", \"shijing.json\"),\n",
    "    # 楚辞\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"chuci\", \"chuci.json\"),\n",
    "    # 诗\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"shi\", \"poet*.json\"),\n",
    "    # 词\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"ci\", \"ci*.json\"),\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"nalanxingde\", \"*.json\"),\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"wudai\", \"huajianji\", \"*juan.json\"),\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"wudai\", \"nantang\", \"poetrys.json\"),\n",
    "    # 曲\n",
    "    os.path.join(root, \"raw\", \"chinese-poetry\", \"yuanqu\", \"yuanqu.json\"),\n",
    "]\n",
    "fs = [each for f in fp for each in glob.glob(f)]\n",
    "\n",
    "l5 = []\n",
    "dicts = dict()\n",
    "for fi in fs:\n",
    "    lines = json.load(open(fi, \"r\", encoding=\"utf-8\"))\n",
    "    if isinstance(lines, dict):\n",
    "        lines = [lines]\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"lunyu\" in fi:\n",
    "            author = \"孔子\"\n",
    "            genre = \"经书\"\n",
    "            title = line['chapter']\n",
    "            contents = \"\".join(line['paragraphs'])\n",
    "        elif \"daxue\" in fi:\n",
    "            author = \"曾子\"\n",
    "            genre = \"经书\"\n",
    "            title = \"大学\"\n",
    "            contents = converter.convert(\"\".join(line['paragraphs'])).replace(\"「\", \"“\").replace(\"」\", \"”\")\n",
    "        elif \"mengzi\" in fi:\n",
    "            author = \"孟子\"\n",
    "            genre = \"经书\"\n",
    "            title = converter.convert(line['chapter'])\n",
    "            contents = converter.convert(\"\".join(line['paragraphs'])).replace(\"「\", \"“\").replace(\"」\", \"”\")\n",
    "        elif \"zhongyong\" in fi:\n",
    "            author = \"孔伋\"\n",
    "            genre = \"经书\"\n",
    "            title = \"中庸\"\n",
    "            contents = converter.convert(\"\".join(line['paragraphs'])).replace(\"「\", \"“\").replace(\"」\", \"”\")\n",
    "        elif \"caocao\" in fi:\n",
    "            author = \"曹操\"\n",
    "            genre = \"古体诗\"\n",
    "            title = line['title']\n",
    "            contents = \"\".join(line['paragraphs'])\n",
    "        elif \"shijing\" in fi:\n",
    "            author = \"诗经\"\n",
    "            genre = \"古体诗\"\n",
    "            title = line['chapter'] + \"-\" + line['section'] + \"-\" + line['title']\n",
    "            contents = \"\".join(line['content'])\n",
    "        elif \"chuci\" in fi:\n",
    "            author = line['author']\n",
    "            genre = \"楚辞\"\n",
    "            title = line['section'] + \"-\" + line['title']\n",
    "            contents = \"\".join(line['content'])\n",
    "        elif \"nalanxingde\" in fi:\n",
    "            author = line['author']\n",
    "            genre = \"词\"\n",
    "            title = line['title']\n",
    "            contents = \"\".join(line['para'])\n",
    "        elif \"huajianci\" in fi:\n",
    "            author = line['author']\n",
    "            genre = \"词\"\n",
    "            title = line['title']\n",
    "            contents = \"\".join(line['paragraphs'])\n",
    "        elif \"nantang\" in fi:\n",
    "            author = line['author']\n",
    "            genre = \"词\"\n",
    "            title = line['title']\n",
    "            contents = \"\".join(line['paragraphs'])\n",
    "        elif \"yuanqu\" in fi:\n",
    "            author = line['author']\n",
    "            genre = \"曲\"\n",
    "            title = line['title']\n",
    "            contents = \"\".join(line['paragraphs'])\n",
    "        elif \"shi\" in fi:\n",
    "            if len(line['paragraphs']) <= 0:\n",
    "                continue\n",
    "            author = converter.convert(line['author'])\n",
    "            genre = \"五言诗\" if len(line['paragraphs'][0]) == 12 else \"七言诗\"\n",
    "            title = converter.convert(line['title'])\n",
    "            contents = converter.convert(\"\".join(line['paragraphs']))\n",
    "        elif \"ci\" in fi:\n",
    "            author = line['author']\n",
    "            genre = \"词\"\n",
    "            title = line['rhythmic']\n",
    "            contents = \"\".join(line['paragraphs'])\n",
    "        if genre not in dicts:\n",
    "            dicts[genre] = dict()\n",
    "        if author not in dicts[genre]:\n",
    "            dicts[genre][author] = dict()\n",
    "        quantifier = \"篇\" if genre in [\"经书\", \"楚辞\"] else \"首\"\n",
    "        prompt = f\"以{author}的风格，写一{quantifier}{genre}，题为{title}{tokenizer.sep_token}\"\n",
    "        answers = [{\"answer\": contents, \"score\": 1}]\n",
    "        l5.append({\"prompt\": prompt, \"answers\": answers, \"genre\": genre, \"title\": title, \"author\": author})\n",
    "        dicts[genre][author][title] = contents\n",
    "        \n",
    "t2 = time.time()\n",
    "print(f\"length: {len(l5)}, # different lengths: {len(dicts)}, time taken: {t2-t1} s\")\n",
    "fo = os.path.join(root, \"chatgpt\", \"processed\", \"chinese_poetry.jsonl\")\n",
    "with open(fo, \"w\", encoding=\"utf-8\") as w:\n",
    "    for i, l in tqdm(enumerate(l5), desc=\"Processing Chinese Poetry\"):\n",
    "        genre = l['genre']\n",
    "        author = l['author']\n",
    "        title = l['title']\n",
    "        prompt = l['prompt']\n",
    "        answers = l['answers']\n",
    "        # 同作者其他作品-2\n",
    "        titles_tmp = set(dicts[genre][author].keys())\n",
    "        titles_tmp.remove(title)\n",
    "        if len(titles_tmp) > 0:\n",
    "            t = random.choice(list(titles_tmp))\n",
    "            answers.append({\"answer\": dicts[genre][author][t], \"score\": 0})\n",
    "        # 同体裁其他作者其他作品-1\n",
    "        authors_tmp = set(dicts[genre].keys())\n",
    "        authors_tmp.remove(author)\n",
    "        a = random.choice(list(authors_tmp))\n",
    "        t = random.choice(list(dicts[genre][a].keys()))\n",
    "        answers.append({\"answer\": dicts[genre][a][t], \"score\": -1})\n",
    "        # 不同体裁作品-0\n",
    "        genres_tmp = set(dicts.keys())\n",
    "        genres_tmp.remove(genre)\n",
    "        g = random.choice(list(genres_tmp))\n",
    "        a = random.choice(list(dicts[g].keys()))\n",
    "        t = random.choice(list(dicts[g][a].keys()))\n",
    "        answers.append({\"answer\": dicts[g][a][t], \"score\": -2})\n",
    "        w.write(json.dumps({\"prompt\": prompt, \"answers\": answers}, ensure_ascii=False)+'\\n')\n",
    "print(f\"length: {i}, time taken: {time.time()-t2} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baike_qa_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1425170\n"
     ]
    }
   ],
   "source": [
    "f = os.path.join(root, \"raw\", \"baike_qa_train.json\")\n",
    "items = []\n",
    "lens_prompt = []\n",
    "lens_label = []\n",
    "with open(f, \"r\", encoding=\"utf-8\") as r:\n",
    "    while True:\n",
    "        line = r.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        item = json.loads(line.strip(\"\\n\"))\n",
    "        prompt = clean_text(item['title'] if len(item['title']) > len(item['desc']) else item['desc'])\n",
    "        label = clean_text(item['answer'])\n",
    "        items.append(item)\n",
    "        lens_prompt.append(len(prompt))\n",
    "        lens_label.append(len(label))\n",
    "print(len(items))\n",
    "print(np.percentile(lens_prompt, np.arange(90, 101)))\n",
    "print(np.percentile(lens_label, np.arange(90, 101)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rm-static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5103, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nHuman: I am trying to write a fairy tale. ...</td>\n",
       "      <td>This sounds like a really interesting modern ...</td>\n",
       "      <td>This sounds like a really interesting modern ...</td>\n",
       "      <td>And the prince and the princess both decide t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nHuman: What flowers should I grow to attra...</td>\n",
       "      <td>Great, there are a lot of different kinds of ...</td>\n",
       "      <td>Great, there are a lot of different kinds of ...</td>\n",
       "      <td>In particular, it’s important to have a wide ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  \\n\\nHuman: I am trying to write a fairy tale. ...   \n",
       "1  \\n\\nHuman: What flowers should I grow to attra...   \n",
       "\n",
       "                                            response  \\\n",
       "0   This sounds like a really interesting modern ...   \n",
       "1   Great, there are a lot of different kinds of ...   \n",
       "\n",
       "                                              chosen  \\\n",
       "0   This sounds like a really interesting modern ...   \n",
       "1   Great, there are a lot of different kinds of ...   \n",
       "\n",
       "                                            rejected  \n",
       "0   And the prince and the princess both decide t...  \n",
       "1   In particular, it’s important to have a wide ...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi = os.path.join(root, \"raw\", \"rm-static\", \"data\", \"test-00000-of-00001-bf4c733542e35fcb.parquet\")\n",
    "df = pd.read_parquet(fi)\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"模型回答：\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTPanguForCausalLM(\n",
       "  (transformer): GPTPanguModel(\n",
       "    (wte): Embedding(40000, 2560)\n",
       "    (wpe): Embedding(1024, 2560)\n",
       "    (wqe): Embedding(1024, 2560)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (28): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (29): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (30): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (31): GPTPanguBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTPanguAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (c_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTPanguMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=40000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, use_cache=False)\n",
    "model.resize_token_embeddings(len(tokenizer.sp))\n",
    "model.config.end_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.max_length_prompt = 200\n",
    "model.to(device)\n",
    "# print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_files = os.path.join(root, \"Data\", \"chatgpt\", \"output\", \"sft\", \"pangu-350M\", \"checkpoint-12000\", \"pytorch_model*.bin\")\n",
    "# checkpoint_files = os.path.join(root, \"Data\", \"output\", \"sft\", \"pangu-2.6B\", \"checkpoint-9000\", \"pytorch_model*.bin\")\n",
    "checkpoints = glob.glob(checkpoint_files)\n",
    "st = dict()\n",
    "for checkpoint in checkpoints:\n",
    "    st.update(torch.load(checkpoint, map_location=\"cpu\"))\n",
    "model.load_state_dict(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'GPTPanguForCausalLM' is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GPT2LMHeadModel', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel'].\n"
     ]
    }
   ],
   "source": [
    "text_generator = TextGenerationPipeline(model, tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = os.path.join(root, \"Data\", \"raw\", \"baike_qa_train.json\")\n",
    "i = 0\n",
    "prompts = []\n",
    "prompts_processed = []\n",
    "labels = []\n",
    "with open(f, \"r\", encoding=\"utf-8\") as r:\n",
    "    while True:\n",
    "        line = r.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        item = json.loads(line.strip(\"\\n\"))\n",
    "        prompt = clean_text(item['title'] if len(item['title']) > len(item['desc']) else item['desc'])\n",
    "        label = clean_text(item['answer'])\n",
    "        prompt_processed = prompt + tokenizer.sep_token + prefix\n",
    "        prompts.append(prompt)\n",
    "        prompts_processed.append(prompt_processed)\n",
    "        labels.append(label)\n",
    "        i += 1\n",
    "        if i > 1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: 有6区无尽之海众神之子工会的吗?你们会怎么了?看不见人了,解散了 \n",
      "label: 我听原众神成员说 众神的副会长 卷了工会财产跑路了(几千W呢)  所以导致工会解散 哎 真是人心陷恶   \n",
      "model answer-0: 我也是无尽之海的,现在工会还没解散,是不可能解散了,不过我想你应该有很多个工会吧,应该有很多人在等着你,如果你有足够的耐心的话,可以去工会看看有多少人在等着你,我有个朋友就在等着我,我说的是在无尽之海,你知道的,要是没人的话,你就去工会看看,反正现在工会也没解散,你可以去公会看看,我也是在无尽之海的,在无尽之海也有很多人在等着你,要是你能坚持的话,我想你会更好的!结舌\n",
      "model answer-1: 现在工会也解散了,是吧··タタタ瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄瞄\n",
      "model answer-2: 你可以去看看万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛万盛NdF万盛NdF\n",
      "model answer-3: 不解散了,我是6区无尽之海的,我想知道,如果解散的话,工会的人和工会的人一起走,会怎么样?方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒\n",
      "model answer-4: 有,因为工会的人太多了,服务器都快封了。タ榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "\n",
      "\n",
      "\n",
      "prompt: 老师好，青青稞酒明天再跌就可以建仓了吗？谢谢。 \n",
      "label: 青青稞酒：散户在不断地卖出，主力资金却在周三悄悄地吃便宜货。该股处于上市以来的最低迷状态，这一点从成交量就可以看得清清楚楚。但是，现在也许并不是你介入的最佳时机。因为从形态看，散户还会有更便宜的筹码在以后卖出，而主力资金至今收集的筹码也不足以使其达到建完仓的地步。因此建议你再等等。这只股票将来看好是不言而喻的。对于短线操作来说，介入的标志是放量。对于中长线来说，还要看大趋势。\n",
      "model answer-0: 您好! 青青稞酒的主力成本是16.5元,目前股价已经下跌到了成本价,如果明天继续下跌,将有可能跌破成本价,建议您可以考虑在回调到成本价附近介入,有可能获利。榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "model answer-1: 如果你的资金不多,建议观望,再来一次,我给你推荐一只股票,就是中信证券,我买过,现在是10.4元,现在还在下跌,还可以建仓,我昨天就告诉你了,你可以去看看。ativelady>ativelady>ativelady>ativelady>ativelady>ativelady>ativelady>ativelady>ativelady>ative\n",
      "model answer-2: 我建议明天再跌,再建仓,到了明天再买周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清\n",
      "model answer-3: 建仓可以,但要控制仓位.周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清周维清\n",
      "model answer-4: 明天再跌就可以建仓了,这是我的看法,你可以参考。榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "\n",
      "\n",
      "\n",
      "prompt: onthesouthofthecity是在城内还是成外？ \n",
      "label: 在城外.例如: Viet Nam is on the south of China.越南在中国的南面. 摘录自英汉综合大词典,该字典还有这样两个例句: Spain is in the south of Europe.西班牙在欧洲的南部。Mexico is to the south of the USA. 墨西哥在美国的南部。\n",
      "model answer-0: the city                                           益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰<\n",
      "model answer-1: 在城内,在成外.在城内,在成外.NdFequipmentNdFequipmentNdFequipmentNdFequipment NdFequipmentNdFequipment NdFequipmentNdFequipment NdFequipment NdFequipment NdF\n",
      "model answer-2: 在城内吧!官宣咯!官宣咯!官宣咯!官宣咯!官宣咯!官宣咯!官宣咯!戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛\n",
      "model answer-3: It's not only the city.榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "model answer-4: 当然是在城里了。。。益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰益彰\n",
      "\n",
      "\n",
      "\n",
      "prompt: 古代官职升迁又怎么说的，比如：拜。。。最好还有例句！\n",
      "label: 古代官职的任免升迁常用有以下词语：①拜：按一定的授予某种官职或名位；任命。例如柳宗元的《封建论》：“朝拜而不到，夕斥之矣。”②除：任命；拜官。例如《史记·魏其武安侯列传》：“上乃曰：‘君除吏已尽未？吾亦欲除吏。’”③迁：调动官职。一般指提升。例如《后汉书·张衡传》：“拜郎中，再迁为太史令。”迁又为放逐；流放。例如柳宗元的《封建论》：“及夫人逆不道，然后掩捕而迁之。”［左迁］：降职。例如《汉书·周昌传》：“吾极知其左迁。”“左迁”还特指贬官在外。例如刘禹锡《初至长安》诗：“左迁凡二纪，重见帝城春。”［迁客］：被贬到外地的官。例如范仲淹《岳阳楼记》：“迁客骚人，多会于此。”④擢：选拔；提拔。例如《汉书·霍光传》：“擢郎为九江太守。”古代平级调动官职叫“转”或“徙”。例如《后汉书·张衡传》：“顺帝初，再转复为太史令，衡不慕当世，所居之官积年不徙。”⑤黜：废；贬退；罢免或降职。例如柳宗元的《封建论》：“有罪得以黜，有能得以赏。”［黜陟］：（官吏）降级或升等。例如《书·舜典》：“三载考绩，三考黜陟幽明。”⑥谪：贬谪；（官吏）因罪被降职并外放。例如柳宗元的《愚溪诗序》：“余以愚触罪，谪潇水上。”⑦出：（官吏）外放；外调。就是京城的官调到地方任职。例如柳宗元的《哭连州凌员外司马》诗：“出守乌江浒。”⑧乞骸骨。封建社会，大臣年老了请求辞职为“乞骸骨”，意思是请求赐还自己的身体，回到家乡。 \n",
      "model answer-0: 古代官职升迁又怎么说的,比如:拜。。。最好还有例句!官宣   官职升迁                                                               \n",
      "model answer-1:                                                                                   \n",
      "model answer-2: 古代官职升迁又怎么说的,比如:拜。。。最好还有例句!榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "model answer-3: 古代官职升迁又怎么说的,比如:拜。。。最好还有例句!KbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKbKb\n",
      "model answer-4: 官职升迁又怎么说的,比如:拜。。。最好还有例句!戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛\n",
      "\n",
      "\n",
      "\n",
      "prompt: 在倾斜的离心管中加入和密度小于水的小球，为什么小球会下降？请给出解题步骤。\n",
      "label: 题目应该是这样: 在倾斜的离心管中加入水和密度小于水的小球，在离心管转动时,为什么小球会下降(向转动轴方向运动)？解:在水中设有一个体积与小球相同的水球，这个水球与小球在同一位置（到转动轴的距离相同）上，由于水的密度大于小球的密度，所以水球的质量大于小球的质量，在离心管转动时水球能够保持不动，满足水球受到的合力提供向心力；而这时小球受到的合力与水球相同但是其质量小于水球，其向心力F=mv^2/r,小于水球需要的向心力，所以有小球受到的合力大于向心力，小球将做向心运动，最终运动到距离转动轴最近处。\n",
      "model answer-0: 根据小球的运动状态可知,小球的运动速度和质量成正比,即质量大的小球运动速度越大,小球速度越大,那么小球的质量越大,小球下降的可能性就越小。榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "model answer-1: 离心管内的小球因重力而产生的离心力是使小球静止的,所以小球在离心管内的重力是使小球下降的,所以小球会下降.方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒\n",
      "model answer-2: 重力是小球的重力,小球的重力是水的重力。小球的重力是水的重力。小球的重力是水的重力。タタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタタ夏亚タ夏亚夏亚夏亚夏亚夏亚夏亚夏亚<e\n",
      "model answer-3: 由于小球体积较小,在离心力的作用下,小球会做匀速直线运动,小球的运动方向与水的运动方向一致,所以小球会下降.749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749749 749749749749749749749749749749749749749749\n",
      "model answer-4: 在倾斜的离心管中加入和密度等于水的小球,为什么小球会下降?                                                                          \n",
      "\n",
      "\n",
      "\n",
      "prompt: 邯郸市哪里有卖纳威沙发 \n",
      "label: 邯郸市亚森家具城内有卖的！ 地址：邯郸市中华大街天客隆超市对过即到！\n",
      "model answer-0: 去邯郸市家具厂有的                                                                                   \n",
      "model answer-1: 我家在邯郸市南环路上,我在邯郸市火车站附近,你可以去看看。榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "model answer-2: 邯郸市东环路的金源时代广场那里有的卖蔺蔺蔺蔺蔺蔺蔺戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛\n",
      "model answer-3: 现在好多的沙发店都有,你可以去看看,或者去他们家看看,他们家的沙发质量都很好,而且价格也很实惠,价格在200元到300元之间,我就是在他们家买的沙发,质量很好,价格在200元到300元之间,你可以去看看,或者去他们家看看,他们家的沙发质量都很好,价格也很实惠,价格在200元到300元之间,你可以去看看,或者去他们家看看,他们家的沙发质量都很好,价格也很实惠,价格在200元到300元之间,你可以去看看,或者去他们家看看,他们家的沙发质量都很好,价格也很实惠,价格在200元到300元之间,你可以去看看,或者去他们家看看,\n",
      "model answer-4: 有啊,你去看看吧蔺蔺蔺蔺蔺蔺戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛\n",
      "\n",
      "\n",
      "\n",
      "prompt: 如图所示，，A、B是圆的直径的两端，小张在A点，小王在B点，同时出发相向而行，他们在点第一次相遇，C点离A点80米；在D点第二次相遇，D点离B点60米。求这个圆的周长。\n",
      "label: 可设方程解答：设圆周长为X米，则得方程，80*2=60+X/2-80X=360（米）（第一次相遇，小张、小王合走半个周长；第二次相遇，小张、小王合走一个周长，即比第一次多走一培路程。这样，小张第一次走路程乘2就等于小张第二次走路程）你的解也是其中方法之一：第一次相遇，小张、小王合走半个周长AB，其中小张走路程AC是80米；第二次相遇，小张、小王合走一个周长 = CD（小王走的左边部分）+CD（小张走的右边部分）。第二次相遇时，小张、小王合走一个周长，等于第一次相遇时，合走半个周长的两培。所以，小张走的右边部分CD就是第一次走的80米（AC）的两培８０＊２（合走与个人走的按比例同时增加两培）。两次相遇时，小张走的路程加起来就是：AD=AC+CD=８０＊２+８０=８０＊３=２４０Ａ到Ｂ（半周长）距离：２４０－６０＝１８０ 即AB=AD-BC\n",
      "model answer-0: 解:设AB于点C, 求证:                                                        \n",
      "model answer-1:                                                               \n",
      "model answer-2: 图象的意思是:A、B两圆直径的两端,小张在A点,小王在B点,同时出发相向而行,他们在点第一次相遇,C点离A点80米;在D点第二次相遇,D点离B点60米。求这个圆的周长。NdFeBNdFeBNdFeBNdFeB\n",
      "model answer-3: C=πd(2*2)=8π=16榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫榫\n",
      "model answer-4: 解:设圆半径为r,圆心角为α,点A,B为圆的直径,圆心角为β,AB为圆的中点,根据圆的周长公式,得到:                                       \n",
      "\n",
      "\n",
      "\n",
      "prompt: Word文档,一页半文字,怎样调整使其真好布满两页? \n",
      "label: 可以全选后点右键选段落,在里面将字体加大,并且加大行距\n",
      "model answer-0: 1、先将每页的内容复制到记事本,然后点“插入”,在“页眉和页脚”中,将“页码”改为“1”,“页码”下的“间距”改为“1”。2、点击“页面设置”,在“纸张大小”中,将“页码”改为“4”。3、点击“页面设置”,在“纸张大小”中,将“页码”改为“5”。4、点击“页面设置”,在“纸张大小”中,将“页码”改为“7”。5、点击“页面设置”,在“纸张大小”中,将“页码”改为“9”。6、点击“页面设置”,\n",
      "model answer-1: 你好!打开Word后,单击菜单栏“工具”——“选项”——“格式”——“字体”——“横排”——“竖排”,然后将“横排”的前边框选,点“确定”即可。方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒方寒\n",
      "model answer-2: 方法一 :在文档中插入一个空白文档,然后在文本框中输入“1”,点击“插入”,在“文本框”中输入“1”。然后点击“格式”,在“文本框”中输入“1”,点击“确定”。方法二 :在文档中插入一个空白文档,然后在文本框中输入“2”,点击“插入”,在“文本框”中输入“2”。然后点击“格式”,在“文本框”中输入“2”,点击“确定”。 Kbxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "model answer-3: 把页眉页脚都调整成全黑的状态,再到文档中设置页眉页脚即可!戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛戛\n",
      "model answer-4: 你说的是在Word中,在文档的一页上,写一个“√”,然后,在光标处,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再在你的文档上,点一下,再\n",
      "\n",
      "\n",
      "\n",
      "prompt: 武汉江夏分局接到群众举报电话,说在天桥下有两个大炸弹,江夏分局民警与排弹专家一起奔赴现场,在桥梁下发现一个红色布袋,专家与民警小心翼翼的拆开布袋,里面还有几层报纸包裹,民警一层层拆开,最后发现果然是两个大炸弹:4个2和一对王…… 警察表示要严肃追究报警人责任\n",
      "label: 好一个出大事了，好一个炸弹！又着实被你忽悠一下，还以为出什么大事了。\n",
      "model answer-0:                根据《中华人民共和国治安管理处罚法》第23条,对举报人的治安处罚如下:                           \n",
      "model answer-1:                                                     \n",
      "model answer-2: 这就是武昌的“大炸弹”!                                                \n",
      "model answer-3:                                                     \n",
      "model answer-4:                                                     \n",
      "\n",
      "\n",
      "\n",
      "prompt: 考点知识清单一1．地图上的方向（1）一般情况：面对地图，上_____下_____,左西右东（2）有指向标的地图：指向标的箭头指向_____方（3）有经纬网的地图：经线指示_____，纬线指示_____2．方格状或圆弧形的经纬网图（1）辨别南、北纬纬度数值向北递增为北纬纬度数值向南递增为南纬（2）辨别东、西经经度数值向东递增为_____经经度数值向西递增为_____经3．以南北极为中心的经纬网图（1）辨别南北图上标出的地球自转方向呈逆时针，则经纬网中心为_____极图上标出的地球自转方向考点知识清单一1．地图上的方向（1）一般情况：面对地图，上_____下_____,左西右东（2）有指向标的地图：指向标的箭头指向_____方（3）有经纬网的地图：经线指示_____，纬线指示_____2．方格状或圆弧形的经纬网图（1）辨别南、北纬纬度数值向北递增为北纬纬度数值向南递增为南纬（2）辨别东、西经经度数值向东递增为_____经经度数值向西递增为_____经3．以南北极为中心的经纬网图（1）辨别南北图上标出的地球自转方向呈逆时针，则经纬网中心为_____极图上标出的地球自转方向呈顺时针，则经纬网中心为_____极（2）辨别东西根据地球自转方向自西向东，即自转箭头指东东经度数向东增大，向西减小，西经反之4． 地图上的比例尺（1）概念：表示图上距离比实地距离缩小的程度，比例尺=_____距离/实地距离（2）表示形式：数字式_______________、文字式_______________、线段式_______________（3）用途：确定图上两个地点的实际距离（4）大小比较比例尺的分母愈大，比例尺愈_____，表示的范围愈_____，内容愈粗略比例尺的分母愈小，比例尺愈_____，表示的范围愈_____，内容愈详细5．地图上的高低起伏（1）高度表示：_____高度（海拔），其定义_______________、相对高度，其定义_______________（2）地形判读坡度陡缓等高线密集处，坡度越_____等高线稀疏处，坡度越_____地形种类山顶，_________________________盆地，_________________________鞍部，_________________________山脊，_________________________山谷，_________________________\n",
      "label: 考点知识清单一地图1．地图上的方向（1）一般情况：面对地图，上（北）下（南）,左西右东（2）有指向标的地图：指向标的箭头指向（北）方（3）有经纬网的地图：经线指示（南北方向），纬线指示（东西方向）2．方格状或圆弧形的经纬网图（1）辨别南、北纬纬度数值向北递增为北纬纬度数值向南递增为南纬（2）辨别东、西经经度数值向东递增为（东）经经度数值向西递增为（西）经3．以南北极为中心的经纬网图（1）辨别南北图上标出的地球自转方向呈逆时针，则经纬网中心为（北）极图上标出的地球自转方向呈顺时针，则经纬网中心为（南）极（2）辨别东西根据地球自转方向自西向东，即自转箭头指东东经度数向东增大，向西减小，西经反之4． 地图上的比例尺（1）概念：表示图上距离比实地距离缩小的程度，比例尺=图上距离/实地距离（2）表示形式：数字式（1/100)、文字式(1厘米代表100千米）、线段式（不太好画）（3）用途：确定图上两个地点的实际距离（4）大小比较比例尺的分母愈大，比例尺愈（小），表示的范围愈（大），内容愈粗略比例尺的分母愈小，比例尺愈（大），表示的范围愈（小），内容愈详细5．地图上的高低起伏（1）高度表示：（地面）高度（海拔），其定义（地面某个地点高出海平面的垂直距离）、相对高度，其定义（某个地点高出另一地点的垂直距离）（2）地形判读坡度陡缓等高线密集处，坡度越（陡）等高线稀疏处，坡度越（缓）地形种类---------题目意思不懂？山顶，_________________________盆地，_________________________鞍部，_________________________山脊，山谷，不会做啦。初一第一册书有呀。。。再找找呼。。。。。。－ －！\n",
      "model answer-0: 1\n",
      "model answer-1: 1\n",
      "model answer-2: 1\n",
      "model answer-3: 1\n",
      "model answer-4: 1\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_return_sequences = 5\n",
    "i = 10\n",
    "j = 20\n",
    "t1 = time.time()\n",
    "results = text_generator(prompts_processed[i:j], max_length=200, num_return_sequences=num_return_sequences,\n",
    "                         do_sample=True, top_k=50, temperature=10.0)\n",
    "print(f\"Finished prediction, time taken: {time.time()-t1}\")\n",
    "\n",
    "for prompt, res, label in zip(prompts[i:j], results[:(j-i)], labels[i:j]):\n",
    "    print(f\"prompt: {prompt}\\nlabel: {label}\")\n",
    "    for k in range(num_return_sequences):\n",
    "        model_answer = res[k]['generated_text'].split(prefix)[1].replace(\"<eot>\", \"\").replace(\"<pad>\", \"\")\n",
    "        print(f\"model answer-{k}: {model_answer}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "I0224 13:57:47.086069 140704479725184 __init__.py:113] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/wf/12stcn_s6zq56j9h3fnkv5lm0000gn/T/jieba.cache\n",
      "I0224 13:57:47.104202 140704479725184 __init__.py:133] Loading model from cache /var/folders/wf/12stcn_s6zq56j9h3fnkv5lm0000gn/T/jieba.cache\n",
      "Loading model cost 0.565 seconds.\n",
      "I0224 13:57:47.664287 140704479725184 __init__.py:165] Loading model cost 0.565 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "I0224 13:57:47.666184 140704479725184 __init__.py:166] Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: 内有吊车，牛腿高度怎么定？根据什么来定？生产的工艺要求？还是要考虑到其他因素？\n",
      "model answer-0: 工艺性强了定在1.2的高。生产量要能到2。6车就行不生产也有1.6高度牛只等。一般要求1.0就能很明确可以计算:工艺高到多少比较科学(不能超过3的要考虑3吨以下汽车要超过1.0等要求)\n",
      "model answer-1: 这是可以确定重量(1米60一米40多啊牛腿不是固定啊呵呵)当然这有很大差别呢主要工艺性决定,如要求速度很高就要设较佳速度来加工...呵呵了。\n",
      "model answer-2: 牛吊(头向下弯曲至脚尖方向的过程均指同一吊轮形式而分开表示用轮、吊、牛三个量块计量不同头辐。下轮吊运时常由1对,8台1式起重机按一用一吊或两种方式排列配置完成为准\n",
      "model answer-3: 先定一根长100/1*30厘米钢管和高70公厘米无缝钢管和厚4一13之间2组钢管做“架子钢骨柱基础架板基础或柱的下部混凝土(每只立柱混凝土20到1250加500就足够≥1)柱筋2和圈4根\n",
      "model answer-4: 楼上有人是傻不隆科型吧.这个很不好设定啊如果把车架(如W5F和OYFL6ZXHB车架有不小)的强度用公式A2F1×hP定好车臂高/m在1和100之最小就足够大这样\n"
     ]
    }
   ],
   "source": [
    "prompt = \"北大和清华哪个更好？\"\n",
    "prompt_processed = prompt + tokenizer.sep_token + prefix\n",
    "num_return_sequences=5\n",
    "res = text_generator(prompt_processed, max_length=200, num_return_sequences=num_return_sequences,\n",
    "                         do_sample=True, top_k=50, temperature=5.0)\n",
    "print(f\"prompt: {prompt}\")\n",
    "for i in range(num_return_sequences):\n",
    "    model_answer = res[i]['generated_text'].split(prefix)[1].replace(\"<eot>\", \"\").replace(\"<pad>\", \"\")\n",
    "    # print(res)\n",
    "    print(f\"model answer-{i}: {model_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = \"D:\\\\Data\\\\models\\\\pangu-350M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_cache=False, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'unk_token': \"<unk>\",\n",
    "                                  'bos_token': \"<s>\",\n",
    "                                  'eos_token': \"<eot>\",\n",
    "                                  'pad_token': \"<pad>\",\n",
    "                                  \"sep_token\": \"<sep>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\SUNZEY~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.501 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "max_length = 1024\n",
    "text = \"你好，你是谁\"\n",
    "# text = \"<|startoftext|>\" + text + \"<|endoftext|>\"\n",
    "res = tokenizer(text, max_length=max_length, truncation=\"longest_first\", \n",
    "          return_tensors=\"pt\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()\n",
    "# torch.cat((res['input_ids'], res['input_ids']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
