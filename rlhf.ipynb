{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcf0bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, re, random, glob\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    TextGenerationPipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3af929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\"\n",
    "# device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ce0b8",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "088b80f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"D:\\\\Data\\\\models\\\\pangu_2_6B\"\n",
    "# model_name_or_path = \"D:\\\\Data\\\\models\\\\pangu-350M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89bb86fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of GPTPanguForCausalLM were not initialized from the model checkpoint at D:\\Data\\models\\pangu-350M and are newly initialized: ['transformer.h.5.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.23.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.9.attn.bias', 'lm_head.weight', 'transformer.h.17.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.22.attn.bias', 'transformer.h.0.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.19.attn.bias', 'transformer.h.13.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.6.attn.bias', 'transformer.h.16.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.12.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.11.attn.bias', 'transformer.h.17.attn.bias', 'transformer.h.7.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.21.attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, use_cache=False)\n",
    "model.to(device)\n",
    "# print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a49642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# st = torch.load(os.path.join(model_name_or_path, \"pytorch_model.bin\"), map_location=\"cpu\")\n",
    "# print(len(st))\n",
    "# # st.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b438220a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'GPTPanguForCausalLM' is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel'].\n",
      "Setting `pad_token_id` to `eos_token_id`:9 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '中国和美国和日本和法国和加拿大和澳大利亚的首都分别是哪里？\\n中国和美国和日本和法国和加拿大和澳大利亚的首都分别是哪里?<eot>'}]\n"
     ]
    }
   ],
   "source": [
    "text_generator = TextGenerationPipeline(model, tokenizer, device=device)\n",
    "# greedy search\n",
    "print(text_generator(\"中国和美国和日本和法国和加拿大和澳大利亚的首都分别是哪里？\", max_length=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec972ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
